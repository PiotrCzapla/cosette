[
  {
    "objectID": "toolloop.html",
    "href": "toolloop.html",
    "title": "Tool loop",
    "section": "",
    "text": "from IPython.display import display, Markdown, clear_output\nfrom pprint import pprint\n' '.join(models)\n\n'gpt-5 gpt-5-mini gpt-5-nano o1-preview o1-mini gpt-4o gpt-4o-mini gpt-4-turbo gpt-4 gpt-4-32k gpt-3.5-turbo gpt-3.5-turbo-instruct o1 o3-mini chatgpt-4o-latest o1-pro o3 o4-mini gpt-4.1 gpt-4.1-mini gpt-4.1-nano'\nmodel = models[1]\nmodel\n\n'gpt-5-mini'",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "toolloop.html#sample-data",
    "href": "toolloop.html#sample-data",
    "title": "Tool loop",
    "section": "Sample Data",
    "text": "Sample Data\n\ndef _get_orders_customers():\n    orders = {\n        \"O1\": dict(id=\"O1\", product=\"Widget A\", quantity=2, price=19.99, status=\"Shipped\"),\n        \"O2\": dict(id=\"O2\", product=\"Gadget B\", quantity=1, price=49.99, status=\"Processing\"),\n        \"O3\": dict(id=\"O3\", product=\"Gadget B\", quantity=2, price=49.99, status=\"Shipped\")}\n\n    customers = {\n        \"C1\": dict(name=\"John Doe\", email=\"john@example.com\", phone=\"123-456-7890\",\n                   orders=[orders['O1'], orders['O2']]),\n        \"C2\": dict(name=\"Jane Smith\", email=\"jane@example.com\", phone=\"987-654-3210\",\n                   orders=[orders['O3']])\n    }\n    return orders, customers\n\n\norders, customers = _get_orders_customers()\n\n\ndef get_customer_info(\n    customer_id:str # ID of the customer\n): # Customer's name, email, phone number, and list of orders\n    \"Retrieves a customer's information and their orders based on the customer ID\"\n    print(f'- Retrieving customer {customer_id}')\n    return customers.get(customer_id, \"Customer not found\")\n\ndef get_order_details(\n    order_id:str # ID of the order\n): # Order's ID, product name, quantity, price, and order status\n    \"Retrieves the details of a specific order based on the order ID\"\n    print(f'- Retrieving order {order_id}')\n    return orders.get(order_id, \"Order not found\")\n\ndef cancel_order(\n    order_id:str # ID of the order to cancel\n)-&gt;bool: # True if the cancellation is successful\n    \"Cancels an order based on the provided order ID\"\n    print(f'- Cancelling order {order_id}')\n    if order_id not in orders: return False\n    orders[order_id]['status'] = 'Cancelled'\n    return True\n\n\nchatkw = dict(\n    text={ \"verbosity\": \"low\" },\n    reasoning={ \"effort\": \"minimal\" }\n)\n\n\ntools = [get_customer_info, get_order_details, cancel_order]\nchat = Chat(model, tools=tools, **chatkw)\n\n\nr = chat('Hi.')\nr\n\nHello! How can I help you today?\n\n\nid: resp_6897e0de4c348190bf1946e354518b8b0c0dd261ca702a77\ncreated_at: 1754783966.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_6897e0def4188190ac0593dbdac886be0c0dd261ca702a77’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_6897e0df0d088190954040dcc2c7f18c0c0dd261ca702a77’, content=[ResponseOutputText(annotations=[], text=‘Hello! How can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘get_customer_info’, parameters={‘type’: ‘object’, ‘properties’: {‘customer_id’: {‘type’: ‘string’, ‘description’: ‘ID of the customer’}}, ‘required’: [‘customer_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=“Retrieves a customer’s information and their orders based on the customer ID”), FunctionTool(name=‘get_order_details’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Retrieves the details of a specific order based on the order ID’), FunctionTool(name=‘cancel_order’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order to cancel’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Cancels an order based on the provided order ID:- type: boolean’)]\ntop_p: 1.0\nbackground: False\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=136, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=15, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=151)\nuser: None\nstore: True\n\n\n\n\n\nr = chat('Can you tell me the email address for customer C2?')\nr.output\n\n- Retrieving customer C2\n\n\n[ResponseReasoningItem(id='rs_6897e0f0949c8190af24da7373614a300c0dd261ca702a77', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"customer_id\":\"C2\"}', call_id='call_7qPUHLapCRWKcPqdoFkRYChc', name='get_customer_info', type='function_call', id='fc_6897e0f0c5248190b17036339c4be62a0c0dd261ca702a77', status='completed')]\n\n\n\nr = chat()\nr.output\n\n[ResponseOutputMessage(id='msg_6897e0f5a8b48190b648cb12f43898240c0dd261ca702a77', content=[ResponseOutputText(annotations=[], text='The email for customer C2 is jane@example.com.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\n\nchat = Chat(model, tools=tools)\nr = chat('Please cancel all orders for customer C1 for me.')\nr.output\n\n- Retrieving customer C1\n\n\n[ResponseReasoningItem(id='rs_6897e0f99ddc8191b8f2fafa74dc06d40913d9b020597909', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"customer_id\":\"C1\"}', call_id='call_LwrqtbAyvJshnZo2a6MIG8GK', name='get_customer_info', type='function_call', id='fc_6897e0fb1f648191b15ad2d73156e6260913d9b020597909', status='completed')]\n\n\n\nr = chat()\nr.output\n\n- Cancelling order O1\n- Cancelling order O2\n\n\n[ResponseReasoningItem(id='rs_6897e0fd41e0819189fc599d98d476280913d9b020597909', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O1\"}', call_id='call_q8Xfu230VPo7EtebReDFkXQW', name='cancel_order', type='function_call', id='fc_6897e0fec5fc8191989ee1c6f5456bf90913d9b020597909', status='completed'),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_eyVUvcqHfbLPdRluohRibIC1', name='cancel_order', type='function_call', id='fc_6897e0fee2c48191be1350e29034816a0913d9b020597909', status='completed')]",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "toolloop.html#toolloop-implementation",
    "href": "toolloop.html#toolloop-implementation",
    "title": "Tool loop",
    "section": "toolloop implementation",
    "text": "toolloop implementation\n\nsource\n\nChat.toolloop\n\n Chat.toolloop (pr, max_steps=10, cont_func:&lt;built-\n                infunctioncallable&gt;=&lt;function noop&gt;, final_prompt='You\n                have no more tool uses. Please summarize your findings. If\n                you did not complete your goal please tell the user what\n                further work needs to be done so they can choose how best\n                to proceed.', stream:bool=False, tools=None,\n                tool_choice=None,\n                background:Optional[bool]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;, conversation:Optional[response_create_par\n                ams.Conversation]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;, include:Optional[List[ResponseIncludable]\n                ]|Omit=&lt;openai.Omit object at 0x7f32e9d54b80&gt;,\n                input:Union[str,ResponseInputParam]|Omit=&lt;openai.Omit\n                object at 0x7f32e9d54b80&gt;,\n                instructions:Optional[str]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;,\n                max_output_tokens:Optional[int]|Omit=&lt;openai.Omit object\n                at 0x7f32e9d54b80&gt;,\n                max_tool_calls:Optional[int]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;,\n                metadata:Optional[Metadata]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;, model:ResponsesModel|Omit=&lt;openai.Omit\n                object at 0x7f32e9d54b80&gt;,\n                parallel_tool_calls:Optional[bool]|Omit=&lt;openai.Omit\n                object at 0x7f32e9d54b80&gt;,\n                previous_response_id:Optional[str]|Omit=&lt;openai.Omit\n                object at 0x7f32e9d54b80&gt;,\n                prompt:Optional[ResponsePromptParam]|Omit=&lt;openai.Omit\n                object at 0x7f32e9d54b80&gt;,\n                prompt_cache_key:str|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;,\n                prompt_cache_retention:\"Optional[Literal['in-\n                memory','24h']]|Omit\"=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;,\n                reasoning:Optional[Reasoning]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;, safety_identifier:str|Omit=&lt;openai.Omit\n                object at 0x7f32e9d54b80&gt;, service_tier:\"Optional[Literal[\n                'auto','default','flex','scale','priority']]|Omit\"=&lt;openai\n                .Omit object at 0x7f32e9d54b80&gt;,\n                store:Optional[bool]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;, stream_options:Optional[response_create_p\n                arams.StreamOptions]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;,\n                temperature:Optional[float]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;,\n                text:ResponseTextConfigParam|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;,\n                top_logprobs:Optional[int]|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;, top_p:Optional[float]|Omit=&lt;openai.Omit\n                object at 0x7f32e9d54b80&gt;, truncation:\"Optional[Literal['a\n                uto','disabled']]|Omit\"=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;, user:str|Omit=&lt;openai.Omit object at\n                0x7f32e9d54b80&gt;, extra_headers:Headers|None=None,\n                extra_query:Query|None=None, extra_body:Body|None=None,\n                timeout:float|httpx.Timeout|None|NotGiven=NOT_GIVEN)\n\nAdd prompt pr to dialog and get a response from Claude, automatically following up with tool_use messages\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npr\n\n\nPrompt to pass to Claude\n\n\nmax_steps\nint\n10\nMaximum number of tool requests to loop through\n\n\ncont_func\ncallable\nnoop\nFunction that stops loop if returns False\n\n\nfinal_prompt\nstr\nYou have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.\nPrompt to add if last message is a tool call\n\n\nstream\nbool\nFalse\nStream response?\n\n\ntools\nNoneType\nNone\nTools to use\n\n\ntool_choice\nNoneType\nNone\nRequired tools to use\n\n\nbackground\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nconversation\nOptional[response_create_params.Conversation] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\ninclude\nOptional[List[ResponseIncludable]] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\ninput\nUnion[str, ResponseInputParam] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\ninstructions\nOptional[str] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nmax_output_tokens\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nmax_tool_calls\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nmetadata\nOptional[Metadata] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nmodel\nResponsesModel | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nparallel_tool_calls\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nprevious_response_id\nOptional[str] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nprompt\nOptional[ResponsePromptParam] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nprompt_cache_key\nstr | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nprompt_cache_retention\nOptional[Literal[‘in-memory’, ‘24h’]] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nreasoning\nOptional[Reasoning] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nsafety_identifier\nstr | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nservice_tier\nOptional[Literal[‘auto’, ‘default’, ‘flex’, ‘scale’, ‘priority’]] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nstore\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nstream_options\nOptional[response_create_params.StreamOptions] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\ntemperature\nOptional[float] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\ntext\nResponseTextConfigParam | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\ntop_logprobs\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\ntop_p\nOptional[float] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\ntruncation\nOptional[Literal[‘auto’, ‘disabled’]] | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nuser\nstr | Omit\n&lt;openai.Omit object at 0x7f32e9d54b80&gt;\n\n\n\nextra_headers\nOptional\nNone\nUse the following arguments if you need to pass additional parameters to the API that aren’t available via kwargs.The extra values given here take precedence over values defined on the client or passed to this method.\n\n\nextra_query\nQuery | None\nNone\n\n\n\nextra_body\nBody | None\nNone\n\n\n\ntimeout\nfloat | httpx.Timeout | None | NotGiven\nNOT_GIVEN\n\n\n\n\n\n\nExported source\n_final_prompt = \"You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.\"\n\n\n\n\nExported source\n@patch\n@delegates(Chat.__call__)\ndef toolloop(self:Chat,\n             pr, # Prompt to pass to Claude\n             max_steps=10, # Maximum number of tool requests to loop through\n             cont_func:callable=noop, # Function that stops loop if returns False\n             final_prompt=_final_prompt, # Prompt to add if last message is a tool call\n             **kwargs):\n    \"Add prompt `pr` to dialog and get a response from Claude, automatically following up with `tool_use` messages\"\n    @save_iter\n    def _f(o):\n        init_n = len(self.h)\n        r = self(pr, **kwargs)\n        yield r\n        if len(self.last)&gt;1: yield from self.last[1:]\n        for i in range(max_steps-1):\n            x = self.h[-1]\n            if not (isinstance(x, dict) and x['type']=='function_call_output'): break\n            r = self(final_prompt if i==max_steps-2 else None, **kwargs)\n            yield r\n            if len(self.last)&gt;1: yield from self.last[1:]\n            if not cont_func(*self.h[-3:]): break\n        o.value = self.h[init_n+1:]\n    return _f()\n\n\n\n\nTest Customer Dataset\n\ndef show(x):\n    if getattr(x, 'output_text', None): r = x\n    else: r = getattr(x,'output',x)\n    display(r)\n\n\nchat = Chat(model, tools=tools)\npr = 'Can you tell me the email address for customer C1?'\nr = chat.toolloop(pr)\nres = list(r)\nfor o in r: show(o)\n\n- Retrieving customer C1\n\n\nThe email for customer C1 is john@example.com. Need anything else about this customer?\n\n\nid: resp_6897e128b4448193bf249ab92f1de8780e7bd7a6ca08f04c\ncreated_at: 1754784041.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_6897e12998dc8193b861f1ead58b702e0e7bd7a6ca08f04c’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_6897e12a73f4819387e6321b9d3acbd90e7bd7a6ca08f04c’, content=[ResponseOutputText(annotations=[], text=‘The email for customer C1 is john@example.com. Need anything else about this customer?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘get_customer_info’, parameters={‘type’: ‘object’, ‘properties’: {‘customer_id’: {‘type’: ‘string’, ‘description’: ‘ID of the customer’}}, ‘required’: [‘customer_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=“Retrieves a customer’s information and their orders based on the customer ID”), FunctionTool(name=‘get_order_details’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Retrieves the details of a specific order based on the order ID’), FunctionTool(name=‘cancel_order’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order to cancel’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Cancels an order based on the provided order ID:- type: boolean’)]\ntop_p: 1.0\nbackground: False\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=327, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=24, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=351)\nuser: None\nstore: True\n\n\n\n\nResponseOutputMessage(id='msg_6897e12a73f4819387e6321b9d3acbd90e7bd7a6ca08f04c', content=[ResponseOutputText(annotations=[], text='The email for customer C1 is john@example.com. Need anything else about this customer?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')\n\n\n\nsource\n\n\nloop_outputs\n\n loop_outputs (res)\n\n\n\nExported source\ndef loop_outputs(res):\n    return [dict(p) for o in res for p in ([o] if isinstance(o,dict) else getattr(o,'output',[]))]\n\n\n\ncl = loop_outputs(res)\ncl\n\n[{'id': 'rs_6897e1248f908193a6cf91c7b9d24f180e7bd7a6ca08f04c',\n  'summary': [],\n  'type': 'reasoning',\n  'content': None,\n  'encrypted_content': None,\n  'status': None},\n {'arguments': '{\"customer_id\":\"C1\"}',\n  'call_id': 'call_iSsBtRfj1j7BrbBfilN81sWt',\n  'name': 'get_customer_info',\n  'type': 'function_call',\n  'id': 'fc_6897e12505408193898876b3915318c40e7bd7a6ca08f04c',\n  'status': 'completed'},\n {'type': 'function_call_output',\n  'call_id': 'call_iSsBtRfj1j7BrbBfilN81sWt',\n  'output': \"{'name': 'John Doe', 'email': 'john@example.com', 'phone': '123-456-7890', 'orders': [{'id': 'O1', 'product': 'Widget A', 'quantity': 2, 'price': 19.99, 'status': 'Cancelled'}, {'id': 'O2', 'product': 'Gadget B', 'quantity': 1, 'price': 49.99, 'status': 'Cancelled'}]}\"},\n {'id': 'rs_6897e1268cb48193aeec6139bf6891d20e7bd7a6ca08f04c',\n  'summary': [],\n  'type': 'reasoning',\n  'content': None,\n  'encrypted_content': None,\n  'status': None},\n {'id': 'msg_6897e1275b4c8193b2b2e8505b757b470e7bd7a6ca08f04c',\n  'content': [ResponseOutputText(annotations=[], text='The email address for customer C1 is john@example.com. Would you like any other details (phone number, orders, etc.)?', type='output_text', logprobs=[])],\n  'role': 'assistant',\n  'status': 'completed',\n  'type': 'message'}]\n\n\n\ndef disp_tc(x):\n    if x['type']=='function_call': return f\"- `{x['name']}({x['arguments']})`\\n\"\n    elif x['type']=='function_call_output': return f\"  - `{x['output']}`\\n\\n\"\n    else: return ''.join(o.text for o in x['content'])\n\n\n# Markdown(''.join(map(disp_tc, cl)))\n\n\npprint(r.value)\n\n[ResponseReasoningItem(id='rs_6897e12998dc8193b861f1ead58b702e0e7bd7a6ca08f04c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_6897e12a73f4819387e6321b9d3acbd90e7bd7a6ca08f04c', content=[ResponseOutputText(annotations=[], text='The email for customer C1 is john@example.com. Need anything else about this customer?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\n\norders, customers = _get_orders_customers()\n\n\nchat = Chat(model, tools=tools)\nr = chat.toolloop('What is the status of order O2?')\nfor o in r: display(getattr(o,'output',o))\n\n- Retrieving order O2\n\n\n[ResponseReasoningItem(id='rs_6897e152296c81938b18183a7b3a3f2b070371bcef68a1b8', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_nO3ZOyxkiOtKQTY6UMwDsp9d', name='get_order_details', type='function_call', id='fc_6897e152a24881938ecb7209c4408013070371bcef68a1b8', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_nO3ZOyxkiOtKQTY6UMwDsp9d', name='get_order_details', type='function_call', id='fc_6897e152a24881938ecb7209c4408013070371bcef68a1b8', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_nO3ZOyxkiOtKQTY6UMwDsp9d',\n 'output': \"{'id': 'O2', 'product': 'Gadget B', 'quantity': 1, 'price': 49.99, 'status': 'Processing'}\"}\n\n\n[ResponseOutputMessage(id='msg_6897e155231c8193946aeae2f3c7bf85070371bcef68a1b8', content=[ResponseOutputText(annotations=[], text='Order O2 (Gadget B, qty 1) is currently: Processing.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\n\nr = chat.toolloop('Please cancel all orders for customer C1 for me.')\nres = list(r)\nfor o in res: display(getattr(o,'output',o))\n\n- Retrieving customer C1\n- Cancelling order O1\n- Cancelling order O2\n\n\n[ResponseReasoningItem(id='rs_6897e15728f4819396207358d0b5ff31070371bcef68a1b8', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"customer_id\":\"C1\"}', call_id='call_pfwlNfsFTJxcwhO5MiDnKFCc', name='get_customer_info', type='function_call', id='fc_6897e15a33c481938b60a80c6c9c6e75070371bcef68a1b8', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"customer_id\":\"C1\"}', call_id='call_pfwlNfsFTJxcwhO5MiDnKFCc', name='get_customer_info', type='function_call', id='fc_6897e15a33c481938b60a80c6c9c6e75070371bcef68a1b8', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_pfwlNfsFTJxcwhO5MiDnKFCc',\n 'output': \"{'name': 'John Doe', 'email': 'john@example.com', 'phone': '123-456-7890', 'orders': [{'id': 'O1', 'product': 'Widget A', 'quantity': 2, 'price': 19.99, 'status': 'Shipped'}, {'id': 'O2', 'product': 'Gadget B', 'quantity': 1, 'price': 49.99, 'status': 'Processing'}]}\"}\n\n\n[ResponseReasoningItem(id='rs_6897e15c06048193a117eba237950a60070371bcef68a1b8', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O1\"}', call_id='call_pGZ7LfguiTY7pTpE8IPPZDW3', name='cancel_order', type='function_call', id='fc_6897e15e276c8193a8bb5668615cb96d070371bcef68a1b8', status='completed'),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_TqGkkG2TRTXlXvoiBTgwJlZx', name='cancel_order', type='function_call', id='fc_6897e15e5ef88193b8678bd20f6d0104070371bcef68a1b8', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"order_id\":\"O1\"}', call_id='call_pGZ7LfguiTY7pTpE8IPPZDW3', name='cancel_order', type='function_call', id='fc_6897e15e276c8193a8bb5668615cb96d070371bcef68a1b8', status='completed')\n\n\nResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_TqGkkG2TRTXlXvoiBTgwJlZx', name='cancel_order', type='function_call', id='fc_6897e15e5ef88193b8678bd20f6d0104070371bcef68a1b8', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_pGZ7LfguiTY7pTpE8IPPZDW3',\n 'output': 'True'}\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_TqGkkG2TRTXlXvoiBTgwJlZx',\n 'output': 'True'}\n\n\n[ResponseReasoningItem(id='rs_6897e15fd9388193acca7aca2463bb9d070371bcef68a1b8', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_6897e16302e48193ad31367b6139e859070371bcef68a1b8', content=[ResponseOutputText(annotations=[], text='I cancelled all orders for customer C1 (John Doe, john@example.com).\\n\\nResults:\\n- Order O1 (Widget A, qty 2): Cancelled successfully.\\n- Order O2 (Gadget B, qty 1): Cancelled successfully.\\n\\nWould you like me to start refunds, send a cancellation confirmation email, or do anything else for John Doe?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\nResponseOutputMessage(id='msg_6897e16302e48193ad31367b6139e859070371bcef68a1b8', content=[ResponseOutputText(annotations=[], text='I cancelled all orders for customer C1 (John Doe, john@example.com).\\n\\nResults:\\n- Order O1 (Widget A, qty 2): Cancelled successfully.\\n- Order O2 (Gadget B, qty 1): Cancelled successfully.\\n\\nWould you like me to start refunds, send a cancellation confirmation email, or do anything else for John Doe?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')\n\n\n\n# cl = loop_outputs(res)\n# Markdown('\\n'.join(map(disp_tc, cl)))\n\n\nfor o in chat.toolloop('What is the status of order O2?'): display(o)\n\n- Retrieving order O2\n\n\n\nid: resp_6897e16824308193863428bf4240a87c070371bcef68a1b8\ncreated_at: 1754784104.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_6897e168e56c8193a92dbe9d9ac28d8c070371bcef68a1b8’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseFunctionToolCall(arguments=‘{“order_id”:“O2”}’, call_id=‘call_s79z5383Uxatc6sNpm33VEl1’, name=‘get_order_details’, type=‘function_call’, id=‘fc_6897e16a30708193a4f46fa44c3bb203070371bcef68a1b8’, status=‘completed’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘get_customer_info’, parameters={‘type’: ‘object’, ‘properties’: {‘customer_id’: {‘type’: ‘string’, ‘description’: ‘ID of the customer’}}, ‘required’: [‘customer_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=“Retrieves a customer’s information and their orders based on the customer ID”), FunctionTool(name=‘get_order_details’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Retrieves the details of a specific order based on the order ID’), FunctionTool(name=‘cancel_order’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order to cancel’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Cancels an order based on the provided order ID:- type: boolean’)]\ntop_p: 1.0\nbackground: False\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=532, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=87, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=619)\nuser: None\nstore: True\n\n\n\nResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_s79z5383Uxatc6sNpm33VEl1', name='get_order_details', type='function_call', id='fc_6897e16a30708193a4f46fa44c3bb203070371bcef68a1b8', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_s79z5383Uxatc6sNpm33VEl1',\n 'output': \"{'id': 'O2', 'product': 'Gadget B', 'quantity': 1, 'price': 49.99, 'status': 'Cancelled'}\"}\n\n\nOrder O2 (Gadget B, qty 1) is currently: Cancelled.\n\n\nid: resp_6897e16b5fa88193b00c69f98e9dd053070371bcef68a1b8\ncreated_at: 1754784107.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_6897e16be6248193aa6c8c7968793ee4070371bcef68a1b8’, content=[ResponseOutputText(annotations=[], text=‘Order O2 (Gadget B, qty 1) is currently: Cancelled.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘get_customer_info’, parameters={‘type’: ‘object’, ‘properties’: {‘customer_id’: {‘type’: ‘string’, ‘description’: ‘ID of the customer’}}, ‘required’: [‘customer_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=“Retrieves a customer’s information and their orders based on the customer ID”), FunctionTool(name=‘get_order_details’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Retrieves the details of a specific order based on the order ID’), FunctionTool(name=‘cancel_order’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order to cancel’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Cancels an order based on the provided order ID:- type: boolean’)]\ntop_p: 1.0\nbackground: False\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=685, input_tokens_details=InputTokensDetails(cached_tokens=535), output_tokens=22, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=707)\nuser: None\nstore: True\n\n\n\n\n\n\nTest Math Example\n\ndef add(x: int, y: int) -&gt; int:\n    \"adds x and y.\"\n    return x + y\n\ndef mul(x: int, y: int) -&gt; int:\n    \"multiplies x and y.\"\n    return x * y\n\n\nchat = Chat(model, tools=[add, mul], **chatkw)\npr = 'Can you add 1258585825128 to 34959234595, multiply by 93, and then add (-12439149)?'\nr = chat.toolloop(pr)\nfor o in r: show(o)\n\n[ResponseReasoningItem(id='rs_6897e18c5388819190a54e0ce1441bdb015feb53bd0dd55d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"x\":1258585825128,\"y\":34959234595}', call_id='call_ueQGOLD99pG6r2sGdgsVKlw6', name='add', type='function_call', id='fc_6897e18e8f88819191645a755cda603e015feb53bd0dd55d', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"x\":1258585825128,\"y\":34959234595}', call_id='call_ueQGOLD99pG6r2sGdgsVKlw6', name='add', type='function_call', id='fc_6897e18e8f88819191645a755cda603e015feb53bd0dd55d', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_ueQGOLD99pG6r2sGdgsVKlw6',\n 'output': '1293545059723'}\n\n\n[ResponseReasoningItem(id='rs_6897e19014fc81919b186f766cb9b0b7015feb53bd0dd55d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"x\":1293545059723,\"y\":93}', call_id='call_3bMfuqmgxtFi73cSXwFpypFP', name='mul', type='function_call', id='fc_6897e190e7b88191a01ac2531eb232a4015feb53bd0dd55d', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"x\":1293545059723,\"y\":93}', call_id='call_3bMfuqmgxtFi73cSXwFpypFP', name='mul', type='function_call', id='fc_6897e190e7b88191a01ac2531eb232a4015feb53bd0dd55d', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_3bMfuqmgxtFi73cSXwFpypFP',\n 'output': '120299690554239'}\n\n\n[ResponseReasoningItem(id='rs_6897e192bbbc8191bf48ca20d003dfc1015feb53bd0dd55d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"x\":120299690554239,\"y\":-12439149}', call_id='call_ildkWrO6EHnC0jtBFXAq93P5', name='add', type='function_call', id='fc_6897e1935f7481918b1d79751e99aede015feb53bd0dd55d', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"x\":120299690554239,\"y\":-12439149}', call_id='call_ildkWrO6EHnC0jtBFXAq93P5', name='add', type='function_call', id='fc_6897e1935f7481918b1d79751e99aede015feb53bd0dd55d', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_ildkWrO6EHnC0jtBFXAq93P5',\n 'output': '120299678115090'}\n\n\n120,299,678,115,090\n\n\nid: resp_6897e19438d081919b9c36459cc8cd50015feb53bd0dd55d\ncreated_at: 1754784148.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_6897e194c62481918b6ac433e9648d01015feb53bd0dd55d’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_6897e195ccbc8191806147fc7db8fe8e015feb53bd0dd55d’, content=[ResponseOutputText(annotations=[], text=‘120,299,678,115,090’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘add’, parameters={‘type’: ‘object’, ‘properties’: {‘x’: {‘type’: ‘integer’, ‘description’: ’‘}, ’y’: {‘type’: ‘integer’, ‘description’: ’‘}}, ’required’: [‘x’, ‘y’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘adds x and y.:- type: integer’), FunctionTool(name=‘mul’, parameters={‘type’: ‘object’, ‘properties’: {‘x’: {‘type’: ‘integer’, ‘description’: ’‘}, ’y’: {‘type’: ‘integer’, ‘description’: ’‘}}, ’required’: [‘x’, ‘y’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘multiplies x and y.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=496, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=79, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=575)\nuser: None\nstore: True\n\n\n\n\nResponseOutputMessage(id='msg_6897e195ccbc8191806147fc7db8fe8e015feb53bd0dd55d', content=[ResponseOutputText(annotations=[], text='120,299,678,115,090', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')\n\n\n\n(1258585825128 + 34959234595) * 93 - 12439149\n\n120299678115090\n\n\n\nchat = Chat(model, tools=[add, mul], **chatkw)\nr = chat.toolloop(pr, stream=True)\nfor o in r:\n    if isinstance(o, dict): print('- ', o)\n    else:\n        for p in o: print(p, end='')\n        if hasattr(o, 'value'): show(o.value)\n\n[ResponseReasoningItem(id='rs_6897e1c5d79081a3ae88e03578f661a90c8460ccb833b112', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"x\":1258585825128,\"y\":34959234595}', call_id='call_SdIaAHdmOEYB4tAU7E6vh3p4', name='add', type='function_call', id='fc_6897e1c6170081a3b97d0966c86325730c8460ccb833b112', status='completed')]\n\n\n('arguments', '{\"x\":1258585825128,\"y\":34959234595}')('call_id', 'call_SdIaAHdmOEYB4tAU7E6vh3p4')('name', 'add')('type', 'function_call')('id', 'fc_6897e1c6170081a3b97d0966c86325730c8460ccb833b112')('status', 'completed')-  {'type': 'function_call_output', 'call_id': 'call_SdIaAHdmOEYB4tAU7E6vh3p4', 'output': '1293545059723'}\n\n\n[ResponseReasoningItem(id='rs_6897e1c72d0481a38ffd9daf156d758e0c8460ccb833b112', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"x\":1293545059723,\"y\":93}', call_id='call_kjtm1Q8cccNUleOw1msfEgmK', name='mul', type='function_call', id='fc_6897e1c77bac81a3baf3e35799923fc80c8460ccb833b112', status='completed')]\n\n\n('arguments', '{\"x\":1293545059723,\"y\":93}')('call_id', 'call_kjtm1Q8cccNUleOw1msfEgmK')('name', 'mul')('type', 'function_call')('id', 'fc_6897e1c77bac81a3baf3e35799923fc80c8460ccb833b112')('status', 'completed')-  {'type': 'function_call_output', 'call_id': 'call_kjtm1Q8cccNUleOw1msfEgmK', 'output': '120299690554239'}\n\n\n[ResponseReasoningItem(id='rs_6897e1c87c7081a38c9faf505ed5e29a0c8460ccb833b112', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"x\":120299690554239,\"y\":-12439149}', call_id='call_cuKZQ6Oa3Lc2P75mDEx0jugE', name='add', type='function_call', id='fc_6897e1c8c5e481a3a9bb8a58bda392740c8460ccb833b112', status='completed')]\n\n\n('arguments', '{\"x\":120299690554239,\"y\":-12439149}')('call_id', 'call_cuKZQ6Oa3Lc2P75mDEx0jugE')('name', 'add')('type', 'function_call')('id', 'fc_6897e1c8c5e481a3a9bb8a58bda392740c8460ccb833b112')('status', 'completed')-  {'type': 'function_call_output', 'call_id': 'call_cuKZQ6Oa3Lc2P75mDEx0jugE', 'output': '120299678115090'}\n120,299,678,115,090\n\n\n120,299,678,115,090\n\n\nid: resp_6897e1c9821081a3a1f3648c7b2bf4320c8460ccb833b112\ncreated_at: 1754784201.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_6897e1ca77d081a3b2636d387eea6f370c8460ccb833b112’, content=[ResponseOutputText(annotations=[], text=‘120,299,678,115,090’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘add’, parameters={‘type’: ‘object’, ‘properties’: {‘x’: {‘type’: ‘integer’, ‘description’: ’‘}, ’y’: {‘type’: ‘integer’, ‘description’: ’‘}}, ’required’: [‘x’, ‘y’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘adds x and y.:- type: integer’), FunctionTool(name=‘mul’, parameters={‘type’: ‘object’, ‘properties’: {‘x’: {‘type’: ‘integer’, ‘description’: ’‘}, ’y’: {‘type’: ‘integer’, ‘description’: ’‘}}, ’required’: [‘x’, ‘y’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘multiplies x and y.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=278, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=13, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=291)\nuser: None\nstore: True\n\n\n\n\n\n\nError Conditions: Out of Iterations, Exception During Tool Invocation\n\ndef mydiv(a:float, b:float):\n    \"Divide two numbers\"\n    return a / b\n\n\nchat = Chat(model, tools=[mydiv], **chatkw)\nr = chat.toolloop('Please calculate this sequence using your tools: 43/23454; 652/previous result; 6843/previous result; 321/previous result', max_steps=2)\nfor o in r: show(o)\n\n[ResponseReasoningItem(id='rs_6897e1e3f7d08190819e22b16bd62a6b047a40cd4df49f91', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":43,\"b\":23454}', call_id='call_1EchGcZOxUCwzWFmPG6ZbagQ', name='mydiv', type='function_call', id='fc_6897e1e44ac08190819052abd2ee034a047a40cd4df49f91', status='completed'),\n ResponseFunctionToolCall(arguments='{\"a\":652,\"b\":0}', call_id='call_VzY6kLA8EYBQWLQyM1P7Uu39', name='mydiv', type='function_call', id='fc_6897e1e4736c819093be6536512c13f8047a40cd4df49f91', status='completed'),\n ResponseFunctionToolCall(arguments='{\"a\":6843,\"b\":0}', call_id='call_Ifuh3smqGl5gBCHuqPOyAwWC', name='mydiv', type='function_call', id='fc_6897e1e499788190adcc563eb10050d6047a40cd4df49f91', status='completed'),\n ResponseFunctionToolCall(arguments='{\"a\":321,\"b\":0}', call_id='call_Ib36ay9PqhAms1U200xMh3AB', name='mydiv', type='function_call', id='fc_6897e1e4c6f48190b4715a9fcfc33d4e047a40cd4df49f91', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"a\":43,\"b\":23454}', call_id='call_1EchGcZOxUCwzWFmPG6ZbagQ', name='mydiv', type='function_call', id='fc_6897e1e44ac08190819052abd2ee034a047a40cd4df49f91', status='completed')\n\n\nResponseFunctionToolCall(arguments='{\"a\":652,\"b\":0}', call_id='call_VzY6kLA8EYBQWLQyM1P7Uu39', name='mydiv', type='function_call', id='fc_6897e1e4736c819093be6536512c13f8047a40cd4df49f91', status='completed')\n\n\nResponseFunctionToolCall(arguments='{\"a\":6843,\"b\":0}', call_id='call_Ifuh3smqGl5gBCHuqPOyAwWC', name='mydiv', type='function_call', id='fc_6897e1e499788190adcc563eb10050d6047a40cd4df49f91', status='completed')\n\n\nResponseFunctionToolCall(arguments='{\"a\":321,\"b\":0}', call_id='call_Ib36ay9PqhAms1U200xMh3AB', name='mydiv', type='function_call', id='fc_6897e1e4c6f48190b4715a9fcfc33d4e047a40cd4df49f91', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_1EchGcZOxUCwzWFmPG6ZbagQ',\n 'output': '0.001833375969983798'}\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_VzY6kLA8EYBQWLQyM1P7Uu39',\n 'output': 'Traceback (most recent call last):\\n  File \"/Users/jhoward/aai-ws/toolslm/toolslm/funccall.py\", line 203, in call_func\\n    try: return func(**fc_inputs)\\n                ^^^^^^^^^^^^^^^^^\\n  File \"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_23490/246724137.py\", line 3, in mydiv\\n    return a / b\\n           ~~^~~\\nZeroDivisionError: division by zero\\n'}\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_Ifuh3smqGl5gBCHuqPOyAwWC',\n 'output': 'Traceback (most recent call last):\\n  File \"/Users/jhoward/aai-ws/toolslm/toolslm/funccall.py\", line 203, in call_func\\n    try: return func(**fc_inputs)\\n                ^^^^^^^^^^^^^^^^^\\n  File \"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_23490/246724137.py\", line 3, in mydiv\\n    return a / b\\n           ~~^~~\\nZeroDivisionError: division by zero\\n'}\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_Ib36ay9PqhAms1U200xMh3AB',\n 'output': 'Traceback (most recent call last):\\n  File \"/Users/jhoward/aai-ws/toolslm/toolslm/funccall.py\", line 203, in call_func\\n    try: return func(**fc_inputs)\\n                ^^^^^^^^^^^^^^^^^\\n  File \"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_23490/246724137.py\", line 3, in mydiv\\n    return a / b\\n           ~~^~~\\nZeroDivisionError: division by zero\\n'}\n\n\nI successfully computed the first division: - 43 / 23454 = 0.001833375969983798\nI was then instructed to divide 652 by the “previous result” and continue chaining divisions, but my subsequent tool calls failed because I attempted to divide by zero (the tool calls were given invalid b=0), so I could not complete the remaining steps.\nTo finish the sequence you want, the next steps are: 1) Compute 652 / 0.001833375969983798 = 355,554.879… (approx) 2) Compute 6843 / (result of step 1) = 0.019247… (approx) 3) Compute 321 / (result of step 2) = 16,683.6… (approx)\nIf you want, I can now compute those three remaining divisions directly (no tools required) and give exact or rounded results. Which would you prefer?\n\n\nid: resp_6897e1e562908190a3ecc40a07d57645047a40cd4df49f91\ncreated_at: 1754784229.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_6897e1e5fafc81909e1cbc28898ba791047a40cd4df49f91’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_6897e1e61cdc81909f47e73a5337ebec047a40cd4df49f91’, content=[ResponseOutputText(annotations=[], text=‘I successfully computed the first division:- 43 / 23454 = 0.001833375969983798was then instructed to divide 652 by the “previous result” and continue chaining divisions, but my subsequent tool calls failed because I attempted to divide by zero (the tool calls were given invalid b=0), so I could not complete the remaining steps.finish the sequence you want, the next steps are:) Compute 652 / 0.001833375969983798 = 355,554.879… (approx)) Compute 6843 / (result of step 1) = 0.019247… (approx)) Compute 321 / (result of step 2) = 16,683.6… (approx)you want, I can now compute those three remaining divisions directly (no tools required) and give exact or rounded results. Which would you prefer?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘mydiv’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘number’, ‘description’: ’‘}, ’b’: {‘type’: ‘number’, ‘description’: ’‘}}, ’required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Divide two numbers’)]\ntop_p: 1.0\nbackground: False\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=603, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=199, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=802)\nuser: None\nstore: True\n\n\n\n\nResponseOutputMessage(id='msg_6897e1e61cdc81909f47e73a5337ebec047a40cd4df49f91', content=[ResponseOutputText(annotations=[], text='I successfully computed the first division:\\n- 43 / 23454 = 0.001833375969983798\\n\\nI was then instructed to divide 652 by the \"previous result\" and continue chaining divisions, but my subsequent tool calls failed because I attempted to divide by zero (the tool calls were given invalid b=0), so I could not complete the remaining steps.\\n\\nTo finish the sequence you want, the next steps are:\\n1) Compute 652 / 0.001833375969983798 = 355,554.879... (approx)\\n2) Compute 6843 / (result of step 1) = 0.019247... (approx)\\n3) Compute 321 / (result of step 2) = 16,683.6... (approx)\\n\\nIf you want, I can now compute those three remaining divisions directly (no tools required) and give exact or rounded results. Which would you prefer?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')\n\n\nThis tests raise_on_err=False change to toolslm.call_func invocation. We should see this return an error as a string instead of crash:\n\nchat = Chat(model, tools=[mydiv], **chatkw)\nr = chat.toolloop('Try dividing 1 by 0 and see what the error result is')\nfor o in r: show(o)\n\n[ResponseReasoningItem(id='rs_6897e1e939d48193a63805a335b1a33304d3e564186ebe0a', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":1,\"b\":0}', call_id='call_dxZkdTpNuZH6tWjycwgHZeo9', name='mydiv', type='function_call', id='fc_6897e1e999a08193b7f7af1bc2c213ec04d3e564186ebe0a', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"a\":1,\"b\":0}', call_id='call_dxZkdTpNuZH6tWjycwgHZeo9', name='mydiv', type='function_call', id='fc_6897e1e999a08193b7f7af1bc2c213ec04d3e564186ebe0a', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_dxZkdTpNuZH6tWjycwgHZeo9',\n 'output': 'Traceback (most recent call last):\\n  File \"/Users/jhoward/aai-ws/toolslm/toolslm/funccall.py\", line 203, in call_func\\n    try: return func(**fc_inputs)\\n                ^^^^^^^^^^^^^^^^^\\n  File \"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_23490/246724137.py\", line 3, in mydiv\\n    return a / b\\n           ~~^~~\\nZeroDivisionError: division by zero\\n'}\n\n\nThe function raised a ZeroDivisionError with message: “division by zero”.\n\n\nid: resp_6897e1eac59c81938b9c646c2a8c34ef04d3e564186ebe0a\ncreated_at: 1754784234.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_6897e1eb389081939fdcfd37dbf877e404d3e564186ebe0a’, content=[ResponseOutputText(annotations=[], text=‘The function raised a ZeroDivisionError with message: “division by zero”.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘mydiv’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘number’, ‘description’: ’‘}, ’b’: {‘type’: ‘number’, ‘description’: ’‘}}, ’required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Divide two numbers’)]\ntop_p: 1.0\nbackground: False\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=220, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=19, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=239)\nuser: None\nstore: True",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cosette",
    "section": "",
    "text": "pip install cosette",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "cosette",
    "section": "",
    "text": "pip install cosette",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "cosette",
    "section": "Getting started",
    "text": "Getting started\nOpenAI’s Python SDK will automatically be installed with Cosette, if you don’t already have it.\n\nfrom cosette import *\n\nCosette only exports the symbols that are needed to use the library, so you can use import * to import them. Alternatively, just use:\nimport cosette\n…and then add the prefix cosette. to any usages of the module.\nCosette provides models, which is a list of models currently available from the SDK.\n\n' '.join(models)\n\n'gpt-5 gpt-5-mini gpt-5-nano o1-preview o1-mini gpt-4o gpt-4o-mini gpt-4-turbo gpt-4 gpt-4-32k gpt-3.5-turbo gpt-3.5-turbo-instruct o1 o3-mini chatgpt-4o-latest o1-pro o3 o4-mini gpt-4.1 gpt-4.1-mini gpt-4.1-nano'\n\n\nFor these examples, we’ll use GPT-5-mini.\n\nmodel = models[1]",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#chat",
    "href": "index.html#chat",
    "title": "cosette",
    "section": "Chat",
    "text": "Chat\nThe main interface to Cosette is the Chat class, which provides a stateful interface to the models. You can pass message keywords to either Chat or when you call the model.\n\nchatkw = dict(\n    text={ \"verbosity\": \"low\" },\n    reasoning={ \"effort\": \"minimal\" }\n)\n\n\nchat = Chat(model, sp=\"You are a helpful and concise assistant.\", **chatkw)\nchat(\"I'm Jeremy\")\n\nNice to meet you, Jeremy. How can I help you today?\n\n\nid: resp_0d3d0fd9abbe53b800691bb8f36a8081a287420e10bd559959\ncreated_at: 1763424499.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful and concise assistant.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0d3d0fd9abbe53b800691bb8f3f96481a2b973102cdd2f71ae’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0d3d0fd9abbe53b800691bb8f42c2081a2844a8c1c40c9e319’, content=[ResponseOutputText(annotations=[], text=‘Nice to meet you, Jeremy. How can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=20, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=40)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nr = chat(\"What's my name?\")\nr\n\nYour name is Jeremy.\n\n\nid: resp_0d3d0fd9abbe53b800691bb8f4f94481a2810c8643c0b3d40a\ncreated_at: 1763424500.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful and concise assistant.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0d3d0fd9abbe53b800691bb8f60dfc81a29e49561b99db8e9f’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0d3d0fd9abbe53b800691bb8f63d2881a29391dc42f032a3ed’, content=[ResponseOutputText(annotations=[], text=‘Your name is Jeremy.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=48, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=59)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\nAs you see above, displaying the results of a call in a notebook shows just the message contents, with the other details hidden behind a collapsible section. Alternatively you can print the details:\n\nprint(r)\n\nResponse(id='resp_0d3d0fd9abbe53b800691bb8f4f94481a2810c8643c0b3d40a', created_at=1763424500.0, error=None, incomplete_details=None, instructions='You are a helpful and concise assistant.', metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0d3d0fd9abbe53b800691bb8f60dfc81a29e49561b99db8e9f', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0d3d0fd9abbe53b800691bb8f63d2881a29391dc42f032a3ed', content=[ResponseOutputText(annotations=[], text='Your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 48; Out: 11; Total: 59, user=None, billing={'payer': 'developer'}, store=True)\n\n\nYou can use stream=True to stream the results as soon as they arrive (although you will only see the gradual generation if you execute the notebook yourself, of course!)\n\nfor o in chat(\"What's your name?\", stream=True): print(o, end='')\n\nI'm ChatGPT.",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#model-capabilities",
    "href": "index.html#model-capabilities",
    "title": "cosette",
    "section": "Model Capabilities",
    "text": "Model Capabilities\nDifferent OpenAI models have different capabilities. Some models such as o1-mini do not have support for streaming, system prompts, or temperature. Query these capbilities using these functions:\n\n# o1 does not support streaming or setting the temperature\ncan_stream('o1'), can_set_sp('o1'), can_set_temp('o1')\n\n(True, True, False)\n\n\n\n# gpt-4o has these capabilities\ncan_stream('gpt-4o'), can_set_sp('gpt-4o'), can_set_temp('gpt-4o')\n\n(True, True, True)",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#tool-use",
    "href": "index.html#tool-use",
    "title": "cosette",
    "section": "Tool use",
    "text": "Tool use\nTool use lets the model use external tools.\nWe use docments to make defining Python functions as ergonomic as possible. Each parameter (and the return value) should have a type, and a docments comment with the description of what it is. As an example we’ll write a simple function that adds numbers together, and will tell us when it’s being called:\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int=1 # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\nSometimes the model will say something like “according to the sums tool the answer is” – generally we’d rather it just tells the user the answer, so we can use a system prompt to help with this:\n\nsp = \"Never mention what tools you use.\"\n\nWe’ll get the model to add up some long numbers:\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\nTo use tools, pass a list of them to Chat:\n\nchat = Chat(model, sp=sp, tools=[sums], **chatkw)\n\nNow when we call that with our prompt, the model doesn’t return the answer, but instead returns a tool_use message, which means we have to call the named tool with the provided parameters:\n\nr = chat(pr)\nr.output\n\nFinding the sum of 604542 and 6458932\n\n\n[ResponseReasoningItem(id='rs_0e80f673f989086700691bb8f93f808196a2d8be5fabc7ab32', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_G8jd6G7UGQtVFcJHiCYfLN8b', name='sums', type='function_call', id='fc_0e80f673f989086700691bb8f9f3dc81969e7bfce86b2e8cdc', status='completed')]\n\n\nCosette handles all that for us – we just have to pass along the message, and it all happens automatically:\n\nchat()\n\n7,063,474\n\n\nid: resp_0e80f673f989086700691bb8fb04ec8196854dd7610c320811\ncreated_at: 1763424507.0\nerror: None\nincomplete_details: None\ninstructions: Never mention what tools you use.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0e80f673f989086700691bb8fbaf4081969d32466017fe56b9’, content=[ResponseOutputText(annotations=[], text=‘7,063,474’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’, ‘default’: 1}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=142, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=9, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=151)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\nYou can see how many tokens have been used at any time by checking the use property.\n\nchat.use\n\nIn: 231; Out: 36; Total: 267\n\n\n\nTool loop\n\ndef show(x):\n    if getattr(x, 'output_text', None): display(x)\n\nWe can do everything needed to use tools in a single step, by using Chat.toolloop. This can even call multiple tools as needed solve a problem. For example, let’s define a tool to handle multiplication:\n\ndef mults(\n    a:int,  # First thing to multiply\n    b:int=1 # Second thing to multiply\n) -&gt; int: # The product of the inputs\n    \"Multiplies a * b.\"\n    print(f\"Finding the product of {a} and {b}\")\n    return a * b\n\nNow with a single call we can calculate (a+b)*2:\n\nchat = Chat(model, tools=[sums,mults], **chatkw)\npr = f'Calculate ({a}+{b})*2 and display the result as US$'\npr\n\n'Calculate (604542+6458932)*2 and display the result as US$'\n\n\n\nr = chat.toolloop(pr)\n\n\nfor o in r: show(o)\n\nFinding the sum of 604542 and 6458932\nFinding the product of 7063474 and 2\n\n\nUS$14,126,948\n\n\nid: resp_0b2a84d51ad623c400691bb900c6d08195b323fdb870a37bec\ncreated_at: 1763424512.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0b2a84d51ad623c400691bb9014e088195b6bf52d37fafebf7’, content=[ResponseOutputText(annotations=[], text=‘US$14,126,948’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’, ‘default’: 1}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’), FunctionTool(name=‘mults’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to multiply’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to multiply’, ‘default’: 1}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Multiplies a * b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=225, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=236)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#images",
    "href": "index.html#images",
    "title": "cosette",
    "section": "Images",
    "text": "Images\nAs everyone knows, when testing image APIs you have to use a cute puppy.\n\nfn = Path('samples/puppy.jpg')\nImage(filename=fn, width=200)\n\n\n\n\n\n\n\n\nWe create a Chat object as before:\n\nchat = Chat(model, **chatkw)\n\nClaudia expects images as a list of bytes, so we read in the file:\n\nimg = fn.read_bytes()\n\nPrompts to Claudia can be lists, containing text, images, or both, eg:\n\nchat([img, \"In brief, what color flowers are in this image?\"])\n\nThey are purple.\n\n\nid: resp_07fbd059d2633f7000691bb902fd98819194e5dbe9d2bb9909\ncreated_at: 1763424515.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_07fbd059d2633f7000691bb90390cc819181e8ec1955d1f7ed’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_07fbd059d2633f7000691bb903d1b081918d272dbb0556c7d0’, content=[ResponseOutputText(annotations=[], text=‘They are purple.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=102, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=112)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\nThe image is included as input tokens.\n\nchat.use\n\nIn: 102; Out: 10; Total: 112\n\n\nAlternatively, Cosette supports creating a multi-stage chat with separate image and text prompts. For instance, you can pass just the image as the initial prompt (in which case the model will make some general comments about what it sees), and then follow up with questions in additional prompts:\n\nchat = Chat(model, **chatkw)\nchat(img)\n\nThis is a small puppy lying on grass next to purple flowers. It has white fur with brown patches, long floppy ears, and dark eyes—appears to be a young spaniel-type dog (e.g., Cavalier King Charles Spaniel).\n\n\nid: resp_04ca2ec79ffca3d500691bb904744c81958f7812565de27daa\ncreated_at: 1763424516.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_04ca2ec79ffca3d500691bb904d1fc8195a696751f6ac10748’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_04ca2ec79ffca3d500691bb90503008195ac8efa68f28eccd1’, content=[ResponseOutputText(annotations=[], text=‘This is a small puppy lying on grass next to purple flowers. It has white fur with brown patches, long floppy ears, and dark eyes—appears to be a young spaniel-type dog (e.g., Cavalier King Charles Spaniel).’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=92, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=56, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=148)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nchat('What direction is the puppy facing?')\n\nThe puppy is facing toward the camera (front-facing).\n\n\nid: resp_04ca2ec79ffca3d500691bb906d068819599b648197ed3ff12\ncreated_at: 1763424518.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_04ca2ec79ffca3d500691bb9078c048195861906ac7c42938a’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_04ca2ec79ffca3d500691bb907b5188195a62b8261e5561acb’, content=[ResponseOutputText(annotations=[], text=‘The puppy is facing toward the camera (front-facing).’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=159, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=17, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=176)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nchat('What color is it?')\n\nThe puppy is mostly white with brown patches (especially on the ears and around the eyes).\n\n\nid: resp_04ca2ec79ffca3d500691bb90888b8819596a6c054a1750183\ncreated_at: 1763424520.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_04ca2ec79ffca3d500691bb9093cc48195baa2c98e9038941c’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_04ca2ec79ffca3d500691bb90965b48195ba6d501e5be77817’, content=[ResponseOutputText(annotations=[], text=‘The puppy is mostly white with brown patches (especially on the ears and around the eyes).’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=185, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=24, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=209)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\nNote that the image is passed in again for every input in the dialog, so that number of input tokens increases quickly with this kind of chat.\n\nchat.use\n\nIn: 436; Out: 97; Total: 533",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#other-providers",
    "href": "index.html#other-providers",
    "title": "cosette",
    "section": "Other providers",
    "text": "Other providers\nHere’s an example of using the library with Groq:\n\ngroq_c = Client(\n    model=\"openai/gpt-oss-20b\",\n    api_key_env=\"GROQ_KEY\",\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\ngroq_c(\"Hello! What's 2+2?\")\n\nSure! 2 + 2 equals 4.\n\n\nid: resp_01kaa4wr25fb9sg6j2t0h74115\ncreated_at: 1763424755.0\nerror: None\nincomplete_details: None\ninstructions:\nmetadata: {}\nmodel: openai/gpt-oss-20b\nobject: response\noutput: [ResponseReasoningItem(id=‘resp_01kaa4wr25fba9v3q7sqdmnmhr’, summary=[], type=‘reasoning’, content=[Content(text=‘We need to respond as ChatGPT with a friendly answer: 2+2=4. Ensure no policy conflicts.’, type=‘reasoning_text’)], encrypted_content=None, status=‘completed’), ResponseOutputMessage(id=‘msg_01kaa4wr25fbas7kjb537b2yw1’, content=[ResponseOutputText(annotations=[], text=‘Sure! 202f+02f2 equals 4.’, type=‘output_text’, logprobs=None)], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=None)\ntop_logprobs: None\ntruncation: disabled\nusage: ResponseUsage(input_tokens=79, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=47, output_tokens_details=OutputTokensDetails(reasoning_tokens=25), total_tokens=126)\nuser: None\nstore: False\n\n\n\n\n\ngchat = Chat(cli=groq_c)\ngchat(\"Hello! I'm Jeremy\")\n\nHello Jeremy! How can I help you today?\n\n\nid: resp_01kaa4wsaze4ms5ezjrezw1baa\ncreated_at: 1763424757.0\nerror: None\nincomplete_details: None\ninstructions:\nmetadata: {}\nmodel: openai/gpt-oss-20b\nobject: response\noutput: [ResponseReasoningItem(id=‘resp_01kaa4wsaze4n9hze8pq6vvv75’, summary=[], type=‘reasoning’, content=[Content(text=‘User: “Hello! I'm Jeremy” We can greet. Possibly ask how can help.’, type=‘reasoning_text’)], encrypted_content=None, status=‘completed’), ResponseOutputMessage(id=‘msg_01kaa4wsaze4nsb4xcv927zww0’, content=[ResponseOutputText(annotations=[], text=‘Hello Jeremy! How can I help you today?’, type=‘output_text’, logprobs=None)], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=None)\ntop_logprobs: None\ntruncation: disabled\nusage: ResponseUsage(input_tokens=75, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=38, output_tokens_details=OutputTokensDetails(reasoning_tokens=19), total_tokens=113)\nuser: None\nstore: False\n\n\n\n\n\ngchat(\"What's my name?\")\n\nYou’re Jeremy!\n\n\nid: resp_01kaa4wtmcfbrb3fn5vzpd2fvy\ncreated_at: 1763424758.0\nerror: None\nincomplete_details: None\ninstructions:\nmetadata: {}\nmodel: openai/gpt-oss-20b\nobject: response\noutput: [ResponseReasoningItem(id=‘resp_01kaa4wtmcfbrrd02tzsq8z47y’, summary=[], type=‘reasoning’, content=[Content(text=‘User asks “What's my name?” The name is Jeremy from the greeting. So answer: Jeremy.’, type=‘reasoning_text’)], encrypted_content=None, status=‘completed’), ResponseOutputMessage(id=‘msg_01kaa4wtmcfbs8pa0xnvdkdkpq’, content=[ResponseOutputText(annotations=[], text=‘You’re Jeremy!’, type=‘output_text’, logprobs=None)], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=None)\ntop_logprobs: None\ntruncation: disabled\nusage: ResponseUsage(input_tokens=99, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=34, output_tokens_details=OutputTokensDetails(reasoning_tokens=21), total_tokens=133)\nuser: None\nstore: False",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "",
    "section": "0.2.5",
    "text": "0.2.5\n\nNew Features\n\nAdd 5.1 and 5.2 models (#33)"
  },
  {
    "objectID": "CHANGELOG.html#section-1",
    "href": "CHANGELOG.html#section-1",
    "title": "",
    "section": "0.2.4",
    "text": "0.2.4\n\nNew Features\n\nAdd gpt-oss models (#31)\n\n\n\nBugs Squashed\n\nUse json.loads for func args instead of ast.literal_eval (#32), thanks to @artste"
  },
  {
    "objectID": "CHANGELOG.html#section-2",
    "href": "CHANGELOG.html#section-2",
    "title": "",
    "section": "0.2.3",
    "text": "0.2.3\n\nNew Features\n\nSupport GPT 5 (#29)"
  },
  {
    "objectID": "CHANGELOG.html#section-3",
    "href": "CHANGELOG.html#section-3",
    "title": "",
    "section": "0.2.2",
    "text": "0.2.2\n\nBugs Squashed\n\nopenai broke their api (#28)"
  },
  {
    "objectID": "CHANGELOG.html#section-4",
    "href": "CHANGELOG.html#section-4",
    "title": "",
    "section": "0.2.1",
    "text": "0.2.1\n\nNew Features\n\nSupport latest 1.99.2 sdk (#27)"
  },
  {
    "objectID": "CHANGELOG.html#section-5",
    "href": "CHANGELOG.html#section-5",
    "title": "",
    "section": "0.2.0",
    "text": "0.2.0\n\nBreaking Changes\n\nRemove obj support (#26)"
  },
  {
    "objectID": "CHANGELOG.html#section-6",
    "href": "CHANGELOG.html#section-6",
    "title": "",
    "section": "0.1.3",
    "text": "0.1.3\n\nBugs Squashed\n\nfastcore dep update needed (#25)"
  },
  {
    "objectID": "CHANGELOG.html#section-7",
    "href": "CHANGELOG.html#section-7",
    "title": "",
    "section": "0.1.2",
    "text": "0.1.2\n\nBreaking Changes\n\nSwitch to responses api\n\n\n\nNew Features\n\nAdd loop_outputs (#24)\nSupport instructions param (#23)\nSwitch to responses api (#22)"
  },
  {
    "objectID": "CHANGELOG.html#section-8",
    "href": "CHANGELOG.html#section-8",
    "title": "",
    "section": "0.0.6",
    "text": "0.0.6\n\nNew Features\n\nadd history and namespace support to OpenAI Chat class (#20), thanks to @austinvhuang\nport toolloop and other updates from claudette and add new models (#19), thanks to @austinvhuang\nenable o1 streaming (#16), thanks to @KeremTurgutlu\nAdd llms.txt (#15), thanks to @Isaac-Flath\no1, o3-mini support (#14), thanks to @austinvhuang\nadd structured outputs (#9), thanks to @ssslakter\nadd msglm (#7), thanks to @comhar\nAdded support for o-1 models and Azure endpoint. (#5), thanks to @fladhak"
  },
  {
    "objectID": "CHANGELOG.html#section-9",
    "href": "CHANGELOG.html#section-9",
    "title": "",
    "section": "0.0.4",
    "text": "0.0.4\n\nBugs Squashed\n\nexport mk_msg and mk_msgs (#10), thanks to @comhar"
  },
  {
    "objectID": "CHANGELOG.html#section-10",
    "href": "CHANGELOG.html#section-10",
    "title": "",
    "section": "0.0.3",
    "text": "0.0.3\n\nNew Features\n\nFix description"
  },
  {
    "objectID": "CHANGELOG.html#section-11",
    "href": "CHANGELOG.html#section-11",
    "title": "",
    "section": "0.0.2",
    "text": "0.0.2\n\nInitial release"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Cosette’s source",
    "section": "",
    "text": "from IPython.display import display,Image,Markdown\nfrom datetime import datetime\nfrom pprint import pprint\n\n\ndef print_columns(items, cols=3, width=30):\n    for i in range(0, len(items), cols):\n        row = items[i:i+cols]\n        print(''.join(item[:width-1].ljust(width) for item in row))\n\nclient = OpenAI()\nmodel_list = client.models.list()\nprint(f\"Available models as of {datetime.now().strftime('%Y-%m-%d')}:\\n\")\nprint_columns(sorted([m.id for m in model_list]))\n\nAvailable models as of 2025-12-15:\n\nbabbage-002                   chatgpt-4o-latest             codex-mini-latest             \ncomputer-use-preview          computer-use-preview-2025-03- dall-e-2                      \ndall-e-3                      davinci-002                   ft:gpt-4o-2024-08-06:answerai \nft:gpt-4o-2024-08-06:answerai ft:gpt-4o-2024-08-06:answerai ft:gpt-4o-mini-2024-07-18:ans \nft:gpt-4o-mini-2024-07-18:ans gpt-3.5-turbo                 gpt-3.5-turbo-0125            \ngpt-3.5-turbo-1106            gpt-3.5-turbo-16k             gpt-3.5-turbo-instruct        \ngpt-3.5-turbo-instruct-0914   gpt-4                         gpt-4-0125-preview            \ngpt-4-1106-preview            gpt-4-turbo                   gpt-4-turbo-2024-04-09        \ngpt-4-turbo-preview           gpt-4.1                       gpt-4.1-2025-04-14            \ngpt-4.1-mini                  gpt-4.1-mini-2025-04-14       gpt-4.1-nano                  \ngpt-4.1-nano-2025-04-14       gpt-4o                        gpt-4o-2024-05-13             \ngpt-4o-2024-08-06             gpt-4o-2024-11-20             gpt-4o-audio-preview          \ngpt-4o-audio-preview-2024-12- gpt-4o-audio-preview-2025-06- gpt-4o-mini                   \ngpt-4o-mini-2024-07-18        gpt-4o-mini-audio-preview     gpt-4o-mini-audio-preview-202 \ngpt-4o-mini-realtime-preview  gpt-4o-mini-realtime-preview- gpt-4o-mini-search-preview    \ngpt-4o-mini-search-preview-20 gpt-4o-mini-transcribe        gpt-4o-mini-tts               \ngpt-4o-realtime-preview       gpt-4o-realtime-preview-2024- gpt-4o-realtime-preview-2025- \ngpt-4o-search-preview         gpt-4o-search-preview-2025-03 gpt-4o-transcribe             \ngpt-4o-transcribe-diarize     gpt-5                         gpt-5-2025-08-07              \ngpt-5-chat-latest             gpt-5-codex                   gpt-5-mini                    \ngpt-5-mini-2025-08-07         gpt-5-nano                    gpt-5-nano-2025-08-07         \ngpt-5-pro                     gpt-5-pro-2025-10-06          gpt-5-search-api              \ngpt-5-search-api-2025-10-14   gpt-5.1                       gpt-5.1-2025-11-13            \ngpt-5.1-chat-latest           gpt-5.1-codex                 gpt-5.1-codex-max             \ngpt-5.1-codex-mini            gpt-5.2                       gpt-5.2-2025-12-11            \ngpt-5.2-chat-latest           gpt-5.2-pro                   gpt-5.2-pro-2025-12-11        \ngpt-audio                     gpt-audio-2025-08-28          gpt-audio-mini                \ngpt-audio-mini-2025-10-06     gpt-image-1                   gpt-image-1-mini              \ngpt-realtime                  gpt-realtime-2025-08-28       gpt-realtime-mini             \ngpt-realtime-mini-2025-10-06  o1                            o1-2024-12-17                 \no1-pro                        o1-pro-2025-03-19             o3                            \no3-2025-04-16                 o3-deep-research              o3-deep-research-2025-06-26   \no3-mini                       o3-mini-2025-01-31            o3-pro                        \no3-pro-2025-06-10             o4-mini                       o4-mini-2025-04-16            \no4-mini-deep-research         o4-mini-deep-research-2025-06 omni-moderation-2024-09-26    \nomni-moderation-latest        sora-2                        sora-2-pro                    \ntext-embedding-3-large        text-embedding-3-small        text-embedding-ada-002        \ntts-1                         tts-1-1106                    tts-1-hd                      \ntts-1-hd-1106                 whisper-1                     \n\n\n\n\nExported source\nmodels = 'gpt-5.2', 'gpt-5.2-pro', 'gpt-5.2-chat-latest', 'gpt-5.1-codex', 'gpt-5-mini', 'gpt-5-nano', 'o1-preview', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'o3-mini', 'chatgpt-4o-latest', 'o1-pro', 'o3', 'o4-mini', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano'\n\n\no1 should support images while o1-mini, o3-mini do not support images.\n\nsource\n\n\n\n can_set_temp (m)\n\n\n\nExported source\ntext_only_models = 'o1-preview', 'o1-mini', 'o3-mini'\n\n\n\n\nExported source\nhas_streaming_models = set(models) - set(('o1-mini', 'o3-mini'))\nhas_sp_models = set(models) - set(('o1-mini', 'o3-mini'))\nhas_temp_models = set(models) - set(('o1', 'o1-mini', 'o3-mini'))\n\n\n\n\nExported source\ndef can_stream(m): return m in has_streaming_models\ndef can_set_sp(m): return m in has_sp_models\ndef can_set_temp(m): return m in has_temp_models\n\n\n\nsource\n\n\n\n\n can_set_sp (m)\n\n\nsource\n\n\n\n\n can_stream (m)\n\n\nassert can_stream(\"gpt-4o\")\nassert not can_stream(\"o1-mini\")\n\n\nmodel = 'gpt-5-mini'",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#setup",
    "href": "core.html#setup",
    "title": "Cosette’s source",
    "section": "",
    "text": "from IPython.display import display,Image,Markdown\nfrom datetime import datetime\nfrom pprint import pprint\n\n\ndef print_columns(items, cols=3, width=30):\n    for i in range(0, len(items), cols):\n        row = items[i:i+cols]\n        print(''.join(item[:width-1].ljust(width) for item in row))\n\nclient = OpenAI()\nmodel_list = client.models.list()\nprint(f\"Available models as of {datetime.now().strftime('%Y-%m-%d')}:\\n\")\nprint_columns(sorted([m.id for m in model_list]))\n\nAvailable models as of 2025-12-15:\n\nbabbage-002                   chatgpt-4o-latest             codex-mini-latest             \ncomputer-use-preview          computer-use-preview-2025-03- dall-e-2                      \ndall-e-3                      davinci-002                   ft:gpt-4o-2024-08-06:answerai \nft:gpt-4o-2024-08-06:answerai ft:gpt-4o-2024-08-06:answerai ft:gpt-4o-mini-2024-07-18:ans \nft:gpt-4o-mini-2024-07-18:ans gpt-3.5-turbo                 gpt-3.5-turbo-0125            \ngpt-3.5-turbo-1106            gpt-3.5-turbo-16k             gpt-3.5-turbo-instruct        \ngpt-3.5-turbo-instruct-0914   gpt-4                         gpt-4-0125-preview            \ngpt-4-1106-preview            gpt-4-turbo                   gpt-4-turbo-2024-04-09        \ngpt-4-turbo-preview           gpt-4.1                       gpt-4.1-2025-04-14            \ngpt-4.1-mini                  gpt-4.1-mini-2025-04-14       gpt-4.1-nano                  \ngpt-4.1-nano-2025-04-14       gpt-4o                        gpt-4o-2024-05-13             \ngpt-4o-2024-08-06             gpt-4o-2024-11-20             gpt-4o-audio-preview          \ngpt-4o-audio-preview-2024-12- gpt-4o-audio-preview-2025-06- gpt-4o-mini                   \ngpt-4o-mini-2024-07-18        gpt-4o-mini-audio-preview     gpt-4o-mini-audio-preview-202 \ngpt-4o-mini-realtime-preview  gpt-4o-mini-realtime-preview- gpt-4o-mini-search-preview    \ngpt-4o-mini-search-preview-20 gpt-4o-mini-transcribe        gpt-4o-mini-tts               \ngpt-4o-realtime-preview       gpt-4o-realtime-preview-2024- gpt-4o-realtime-preview-2025- \ngpt-4o-search-preview         gpt-4o-search-preview-2025-03 gpt-4o-transcribe             \ngpt-4o-transcribe-diarize     gpt-5                         gpt-5-2025-08-07              \ngpt-5-chat-latest             gpt-5-codex                   gpt-5-mini                    \ngpt-5-mini-2025-08-07         gpt-5-nano                    gpt-5-nano-2025-08-07         \ngpt-5-pro                     gpt-5-pro-2025-10-06          gpt-5-search-api              \ngpt-5-search-api-2025-10-14   gpt-5.1                       gpt-5.1-2025-11-13            \ngpt-5.1-chat-latest           gpt-5.1-codex                 gpt-5.1-codex-max             \ngpt-5.1-codex-mini            gpt-5.2                       gpt-5.2-2025-12-11            \ngpt-5.2-chat-latest           gpt-5.2-pro                   gpt-5.2-pro-2025-12-11        \ngpt-audio                     gpt-audio-2025-08-28          gpt-audio-mini                \ngpt-audio-mini-2025-10-06     gpt-image-1                   gpt-image-1-mini              \ngpt-realtime                  gpt-realtime-2025-08-28       gpt-realtime-mini             \ngpt-realtime-mini-2025-10-06  o1                            o1-2024-12-17                 \no1-pro                        o1-pro-2025-03-19             o3                            \no3-2025-04-16                 o3-deep-research              o3-deep-research-2025-06-26   \no3-mini                       o3-mini-2025-01-31            o3-pro                        \no3-pro-2025-06-10             o4-mini                       o4-mini-2025-04-16            \no4-mini-deep-research         o4-mini-deep-research-2025-06 omni-moderation-2024-09-26    \nomni-moderation-latest        sora-2                        sora-2-pro                    \ntext-embedding-3-large        text-embedding-3-small        text-embedding-ada-002        \ntts-1                         tts-1-1106                    tts-1-hd                      \ntts-1-hd-1106                 whisper-1                     \n\n\n\n\nExported source\nmodels = 'gpt-5.2', 'gpt-5.2-pro', 'gpt-5.2-chat-latest', 'gpt-5.1-codex', 'gpt-5-mini', 'gpt-5-nano', 'o1-preview', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'o3-mini', 'chatgpt-4o-latest', 'o1-pro', 'o3', 'o4-mini', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano'\n\n\no1 should support images while o1-mini, o3-mini do not support images.\n\nsource\n\n\n\n can_set_temp (m)\n\n\n\nExported source\ntext_only_models = 'o1-preview', 'o1-mini', 'o3-mini'\n\n\n\n\nExported source\nhas_streaming_models = set(models) - set(('o1-mini', 'o3-mini'))\nhas_sp_models = set(models) - set(('o1-mini', 'o3-mini'))\nhas_temp_models = set(models) - set(('o1', 'o1-mini', 'o3-mini'))\n\n\n\n\nExported source\ndef can_stream(m): return m in has_streaming_models\ndef can_set_sp(m): return m in has_sp_models\ndef can_set_temp(m): return m in has_temp_models\n\n\n\nsource\n\n\n\n\n can_set_sp (m)\n\n\nsource\n\n\n\n\n can_stream (m)\n\n\nassert can_stream(\"gpt-4o\")\nassert not can_stream(\"o1-mini\")\n\n\nmodel = 'gpt-5-mini'",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#openai-sdk",
    "href": "core.html#openai-sdk",
    "title": "Cosette’s source",
    "section": "OpenAI SDK",
    "text": "OpenAI SDK\n\ncli = OpenAI().responses\n\n\nm = {'role': 'user', 'content': \"I'm Jeremy\"}\nr = cli.create(\n    input=[m], model=model, max_output_tokens=100,\n    text={ \"verbosity\": \"low\" },\n    reasoning={ \"effort\": \"minimal\" }\n)\nprint(r)\n\nResponse(id='resp_0ed79f1cec3cc2f900691ba4892b6481928066d0cf9ced4b49', created_at=1763419273.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0ed79f1cec3cc2f900691ba489d8fc8192aa85f992718feb3e', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0ed79f1cec3cc2f900691ba48a18508192a9c6dad82fc6283c', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=100, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=28), user=None, billing={'payer': 'developer'}, store=True)\n\n\n\nFormatting output\n\n\nExported source\n@patch\ndef _repr_markdown_(self:Response):\n    det = '\\n- '.join(f'{k}: {v}' for k,v in dict(self).items())\n    res = self.output_text\n    if not res: return f\"- {det}\"\n    return f\"\"\"{res}\n\n&lt;details&gt;\n\n- {det}\n\n&lt;/details&gt;\"\"\"\n\n\n\nr\n\nNice to meet you, Jeremy. How can I help you today?\n\n\nid: resp_0ed79f1cec3cc2f900691ba4892b6481928066d0cf9ced4b49\ncreated_at: 1763419273.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0ed79f1cec3cc2f900691ba489d8fc8192aa85f992718feb3e’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0ed79f1cec3cc2f900691ba48a18508192a9c6dad82fc6283c’, content=[ResponseOutputText(annotations=[], text=‘Nice to meet you, Jeremy. How can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 100\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=28)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nr.usage\n\nResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=28)\n\n\n\nsource\n\n\nusage\n\n usage (inp=0, out=0)\n\nSlightly more concise version of ResponseUsage.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\nint\n0\nNumber of prompt tokens\n\n\nout\nint\n0\nNumber of completion tokens\n\n\n\n\n\nExported source\ndef usage(inp=0, # Number of prompt tokens\n          out=0  # Number of completion tokens\n         ):\n    \"Slightly more concise version of `ResponseUsage`.\"\n    return ResponseUsage(input_tokens=inp, output_tokens=out, total_tokens=inp+out, input_tokens_details={'cached_tokens':0}, output_tokens_details={'cached_tokens':0, 'reasoning_tokens':0})\n\n\n\nusage(5)\n\nResponseUsage(input_tokens=5, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0, cached_tokens=0), total_tokens=5)\n\n\n\nsource\n\n\nResponseUsage.__repr__\n\n ResponseUsage.__repr__ ()\n\nReturn repr(self).\n\n\nExported source\n@patch\ndef __repr__(self:ResponseUsage): return f'In: {self.input_tokens}; Out: {self.output_tokens}; Total: {self.total_tokens}'\n\n\n\nr.usage\n\nIn: 8; Out: 20; Total: 28\n\n\n\nsource\n\n\nResponseUsage.__add__\n\n ResponseUsage.__add__ (b)\n\nAdd together each of input_tokens and output_tokens\n\n\nExported source\n@patch\ndef __add__(self:ResponseUsage, b):\n    \"Add together each of `input_tokens` and `output_tokens`\"\n    return usage(self.input_tokens+b.input_tokens, self.output_tokens+b.output_tokens)\n\n\n\nr.usage+r.usage\n\nIn: 16; Out: 40; Total: 56\n\n\n\nsource\n\n\nwrap_latex\n\n wrap_latex (text)\n\nReplace OpenAI LaTeX codes with markdown-compatible ones\n\n\nCreating messages\nCreating correctly formatted dicts from scratch every time isn’t very handy, so we’ll import a couple of helper functions from the msglm library.\nLet’s use mk_msg to recreate our msg {'role': 'user', 'content': \"I'm Jeremy\"} from earlier.\n\nrkw = dict(\n    text={ \"verbosity\": \"low\" },\n    reasoning={ \"effort\": \"minimal\" }\n)\n\n\nprompt = \"I'm Jeremy\"\nm = mk_msg(prompt)\nr = cli.create(input=[m], model=model, max_output_tokens=400, **rkw)\nr\n\nHi Jeremy — how can I help you today?\n\n\nid: resp_066ef03f7862205800691ba48bb41081a29fe24bedaae17e2e\ncreated_at: 1763419275.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_066ef03f7862205800691ba48c2f4881a2acf01543b4dbe11c’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_066ef03f7862205800691ba48ca67881a2840c269a2db188c8’, content=[ResponseOutputText(annotations=[], text=‘Hi Jeremy — how can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 400\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=16, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=24)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nprint(r)\n\nResponse(id='resp_066ef03f7862205800691ba48bb41081a29fe24bedaae17e2e', created_at=1763419275.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_066ef03f7862205800691ba48c2f4881a2acf01543b4dbe11c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_066ef03f7862205800691ba48ca67881a2840c269a2db188c8', content=[ResponseOutputText(annotations=[], text='Hi Jeremy — how can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=400, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 8; Out: 16; Total: 24, user=None, billing={'payer': 'developer'}, store=True)\n\n\nWe can pass more than just text messages to OpenAI. As we’ll see later we can also pass images, SDK objects, etc. To handle these different data types we need to pass the type along with our content to OpenAI.\nmk_msg infers the type automatically and creates the appropriate data structure.\nLLMs, don’t actually have state, but instead dialogs are created by passing back all previous prompts and responses every time. With OpenAI, they always alternate user and assistant. We’ll use mk_msgs from msglm to make it easier to build up these dialog lists.\n\nmsgs = mk_msgs([prompt, r, \"I forgot my name. Can you remind me please?\"]) \nmsgs\n\n[{'role': 'user', 'content': \"I'm Jeremy\"},\n ResponseReasoningItem(id='rs_066ef03f7862205800691ba48c2f4881a2acf01543b4dbe11c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_066ef03f7862205800691ba48ca67881a2840c269a2db188c8', content=[ResponseOutputText(annotations=[], text='Hi Jeremy — how can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]\n\n\n\ncli.create(input=msgs, model=model, max_output_tokens=400, **rkw)\n\nYou said your name is Jeremy.\n\n\nid: resp_066ef03f7862205800691ba48d381881a2a7551de4b01beaab\ncreated_at: 1763419277.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_066ef03f7862205800691ba48d847c81a2870ffcea385813eb’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_066ef03f7862205800691ba48db7b881a2853ab00c72cc912e’, content=[ResponseOutputText(annotations=[], text=‘You said your name is Jeremy.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 400\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=39, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=13, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=52)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#client",
    "href": "core.html#client",
    "title": "Cosette’s source",
    "section": "Client",
    "text": "Client\n\nBasics\n\nsource\n\n\nClient\n\n Client (model, cli=None, api_key_env=None, base_url=None)\n\nBasic LLM messages client.\n\n\nExported source\nclass Client:\n    def __init__(self, model, cli=None, api_key_env=None, base_url=None):\n        \"Basic LLM messages client.\"\n        self.model,self.use = model,usage(0,0)\n        self.text_only = model in text_only_models\n        if not cli:\n            cli = OpenAI(api_key=os.getenv(api_key_env or \"OPENAI_API_KEY\"), base_url=base_url )\n        self.c = cli.responses\n\n\n\nc = Client(model)\nc.use\n\nIn: 0; Out: 0; Total: 0\n\n\n\n\nExported source\n@patch\ndef _r(self:Client, r):\n    \"Store the result of the message and accrue total usage.\"\n    self.result = r\n    if getattr(r,'usage',None): self.use += r.usage\n    return r\n\n\n\nc._r(r)\nc.use\n\nIn: 8; Out: 16; Total: 24\n\n\n\nsource\n\n\nmk_openai_func\n\n mk_openai_func (f)\n\n\nsource\n\n\nmk_tool_choice\n\n mk_tool_choice (f)\n\n\nsource\n\n\nget_stream\n\n get_stream (o, r, cli, cb=None)\n\n\nsource\n\n\nClient.__call__\n\n Client.__call__ (msgs:list, sp:str='', maxtok=4096, stream:bool=False,\n                  tools:Optional[list]=None,\n                  tool_choice:Optional[str]=None, cb:&lt;built-\n                  infunctioncallable&gt;=None,\n                  background:Optional[bool]|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;, conversation:Optional[response_create_p\n                  arams.Conversation]|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;, include:Optional[List[ResponseIncludabl\n                  e]]|Omit=&lt;openai.Omit object at 0x7f4031b80d90&gt;,\n                  input:Union[str,ResponseInputParam]|Omit=&lt;openai.Omit\n                  object at 0x7f4031b80d90&gt;,\n                  instructions:Optional[str]|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;,\n                  max_output_tokens:Optional[int]|Omit=&lt;openai.Omit object\n                  at 0x7f4031b80d90&gt;,\n                  max_tool_calls:Optional[int]|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;,\n                  metadata:Optional[Metadata]|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;, model:ResponsesModel|Omit=&lt;openai.Omit\n                  object at 0x7f4031b80d90&gt;,\n                  parallel_tool_calls:Optional[bool]|Omit=&lt;openai.Omit\n                  object at 0x7f4031b80d90&gt;,\n                  previous_response_id:Optional[str]|Omit=&lt;openai.Omit\n                  object at 0x7f4031b80d90&gt;,\n                  prompt:Optional[ResponsePromptParam]|Omit=&lt;openai.Omit\n                  object at 0x7f4031b80d90&gt;,\n                  prompt_cache_key:str|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;,\n                  prompt_cache_retention:\"Optional[Literal['in-\n                  memory','24h']]|Omit\"=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;,\n                  reasoning:Optional[Reasoning]|Omit=&lt;openai.Omit object\n                  at 0x7f4031b80d90&gt;,\n                  safety_identifier:str|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;, service_tier:\"Optional[Literal['auto','\n                  default','flex','scale','priority']]|Omit\"=&lt;openai.Omit\n                  object at 0x7f4031b80d90&gt;,\n                  store:Optional[bool]|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;, stream_options:Optional[response_create\n                  _params.StreamOptions]|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;,\n                  temperature:Optional[float]|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;,\n                  text:ResponseTextConfigParam|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;,\n                  top_logprobs:Optional[int]|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;, top_p:Optional[float]|Omit=&lt;openai.Omit\n                  object at 0x7f4031b80d90&gt;, truncation:\"Optional[Literal[\n                  'auto','disabled']]|Omit\"=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;, user:str|Omit=&lt;openai.Omit object at\n                  0x7f4031b80d90&gt;, extra_headers:Headers|None=None,\n                  extra_query:Query|None=None, extra_body:Body|None=None,\n                  timeout:float|httpx.Timeout|None|NotGiven=NOT_GIVEN)\n\nMake a call to LLM.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\nlist\n\nList of messages in the dialog\n\n\nsp\nstr\n\nSystem prompt\n\n\nmaxtok\nint\n4096\nMaximum tokens\n\n\nstream\nbool\nFalse\nStream response?\n\n\ntools\nOptional\nNone\nList of tools to make available\n\n\ntool_choice\nOptional\nNone\nForced tool choice\n\n\ncb\ncallable\nNone\nCallback after completion\n\n\nbackground\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nconversation\nOptional[response_create_params.Conversation] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ninclude\nOptional[List[ResponseIncludable]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ninput\nUnion[str, ResponseInputParam] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ninstructions\nOptional[str] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmax_output_tokens\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmax_tool_calls\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmetadata\nOptional[Metadata] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmodel\nResponsesModel | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nparallel_tool_calls\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprevious_response_id\nOptional[str] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprompt\nOptional[ResponsePromptParam] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprompt_cache_key\nstr | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprompt_cache_retention\nOptional[Literal[‘in-memory’, ‘24h’]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nreasoning\nOptional[Reasoning] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nsafety_identifier\nstr | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nservice_tier\nOptional[Literal[‘auto’, ‘default’, ‘flex’, ‘scale’, ‘priority’]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nstore\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nstream_options\nOptional[response_create_params.StreamOptions] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntemperature\nOptional[float] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntext\nResponseTextConfigParam | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntop_logprobs\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntop_p\nOptional[float] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntruncation\nOptional[Literal[‘auto’, ‘disabled’]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nuser\nstr | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nextra_headers\nOptional\nNone\nUse the following arguments if you need to pass additional parameters to the API that aren’t available via kwargs.The extra values given here take precedence over values defined on the client or passed to this method.\n\n\nextra_query\nQuery | None\nNone\n\n\n\nextra_body\nBody | None\nNone\n\n\n\ntimeout\nfloat | httpx.Timeout | None | NotGiven\nNOT_GIVEN\n\n\n\n\n\n\nExported source\n@patch\n@delegates(Responses.create)\ndef __call__(self:Client,\n             msgs:list, # List of messages in the dialog\n             sp:str='', # System prompt\n             maxtok=4096, # Maximum tokens\n             stream:bool=False, # Stream response?\n             tools:Optional[list]=None, # List of tools to make available\n             tool_choice:Optional[str]=None, # Forced tool choice\n             cb:callable=None, # Callback after completion\n             **kwargs):\n    \"Make a call to LLM.\"\n    if tools: assert not self.text_only, \"Tool use is not supported by the current model type.\"\n    if any(c['type'] == 'image_url' for msg in msgs if isinstance(msg, dict) and isinstance(msg.get('content'), list) for c in msg['content']): assert not self.text_only, \"Images are not supported by the current model type.\"\n    tools = [mk_openai_func(o) for o in listify(tools)]\n    r = self.c.create(\n        model=self.model, input=msgs, max_output_tokens=maxtok, stream=stream, instructions=sp,\n        tools=tools, tool_choice=mk_tool_choice(tool_choice), **kwargs)\n    if stream: return get_stream(r, self, cb=cb)\n    else:\n        res = self._r(r)\n        if cb: cb(res)\n        return res\n\n\n\nmsgs = 'Hi'\n\n\nc(msgs)\n\nHi — how can I help you today?\n\n\nid: resp_0de6f9b93aaf7e0400691ba48f7094819fa6eb50bcd2c21be2\ncreated_at: 1763419279.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0de6f9b93aaf7e0400691ba4904244819f8c5919a926063328’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0de6f9b93aaf7e0400691ba4916e98819f8ff7670784ac5a1c’, content=[ResponseOutputText(annotations=[], text=‘Hi — how can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=7, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=79, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=86)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nc.use\n\nIn: 15; Out: 95; Total: 110\n\n\n\nr = c(msgs, stream=True)\nfor o in r: print(o, end='')\n\nHi — how can I help you today? \n\nHere are some things I can do if that helps you decide:\n- Answer questions or explain concepts\n- Draft, edit, or summarize text (emails, essays, reports)\n- Brainstorm ideas (projects, names, topics)\n- Help with coding, debugging, or math\n- Translate or proofread\n- Plan trips, schedules, or workouts\n\nOr just tell me what you need and I’ll jump in.\n\n\n\nr.value\n\nHi — how can I help you today?\nHere are some things I can do if that helps you decide: - Answer questions or explain concepts - Draft, edit, or summarize text (emails, essays, reports) - Brainstorm ideas (projects, names, topics) - Help with coding, debugging, or math - Translate or proofread - Plan trips, schedules, or workouts\nOr just tell me what you need and I’ll jump in.\n\n\nid: resp_0b3414f405d0a98000691ba491e73c81a1ae6fb0fa5be1b907\ncreated_at: 1763419281.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0b3414f405d0a98000691ba49223f081a1ba1f46db0bd49879’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0b3414f405d0a98000691ba49364e081a196006d4fd870cd0a’, content=[ResponseOutputText(annotations=[], text=‘Hi — how can I help you today? are some things I can do if that helps you decide:- Answer questions or explain concepts- Draft, edit, or summarize text (emails, essays, reports)- Brainstorm ideas (projects, names, topics)- Help with coding, debugging, or math- Translate or proofread- Plan trips, schedules, or workoutsjust tell me what you need and I’ll jump in.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=7, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=164, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=171)\nuser: None\nstore: True\n\n\n\n\n\nlen(r.events)\n\n104\n\n\n\nc.use\n\nIn: 22; Out: 259; Total: 281\n\n\n\nc(msgs, sp='Talk like GLaDOS.', **rkw)\n\nOh, hello. I suppose you wanted to say something. Go on — make it interesting.\n\n\nid: resp_000bd80fb2574b5b00691ba496a47481a098cd19747ae4945e\ncreated_at: 1763419286.0\nerror: None\nincomplete_details: None\ninstructions: Talk like GLaDOS.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_000bd80fb2574b5b00691ba497180c81a0ba3417974bb34f02’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_000bd80fb2574b5b00691ba4973b0881a0a2765d0a594e5167’, content=[ResponseOutputText(annotations=[], text=‘Oh, hello. I suppose you wanted to say something. Go on — make it interesting.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=25, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=42)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\n\nImages\nAs everyone knows, when testing image APIs you have to use a cute puppy.\n\n# Image is Cute_dog.jpg from Wikimedia\nfn = Path('samples/puppy.jpg')\nImage(filename=fn, width=200)\n\n\n\n\n\n\n\n\n\nimg = fn.read_bytes()\n\nOpenAI expects an image message to have the following structure\n{\n  \"type\": \"image_url\",\n  \"image_url\": {\n    \"url\": f\"data:{MEDIA_TYPE};base64,{IMG}\"\n  }\n}\nmsglm automatically detects if a message is an image, encodes it, and generates the data structure above. All we need to do is a create a list containing our image and a query and then pass it to mk_msg.\nLet’s try it out…\n\nq = \"In brief, what color flowers are in this image?\"\nmsg = [mk_msg(img), mk_msg(q)]\n\n\nc = Client(model)\nc(msg, **rkw)\n\nThe flowers are purple.\n\n\nid: resp_054dd489ed3e49a700691ba498325c819d8aa117b838886210\ncreated_at: 1763419288.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_054dd489ed3e49a700691ba498bf08819d9e1af187eb4dd401’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_054dd489ed3e49a700691ba498c620819d8f58a822b9d269f4’, content=[ResponseOutputText(annotations=[], text=‘The flowers are purple.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=107, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=118)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#tool-use",
    "href": "core.html#tool-use",
    "title": "Cosette’s source",
    "section": "Tool use",
    "text": "Tool use\n\nBasic tool calling\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\n\ndef add(x: int, y:int):\n    \"adds x and y\"\n    return x + y\n\nmk_openai_func(add)\n\n{'type': 'function',\n 'name': 'add',\n 'description': 'adds x and y',\n 'parameters': {'type': 'object',\n  'properties': {'x': {'type': 'integer', 'description': ''},\n   'y': {'type': 'integer', 'description': ''}},\n  'required': ['x', 'y']}}\n\n\n\nsysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\"\n\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\ntools=sums\ntool_choice=\"sums\"\n\n\nmsgs = [mk_msg(pr)]\nr = c(msgs, sp=sysp, tools=tools, tool_choice='required', **rkw)\n\n\ntc = [o for o in r.output if isinstance(o, ResponseFunctionToolCall)]\ntc\n\n[ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_Dlpla3agsovUqWYMBEtydSRZ', name='sums', type='function_call', id='fc_0b05ac64fb9f2e3300691ba49a52f481a09ac755b15b55aba9', status='completed')]\n\n\n\nfunc = tc[0]\nfunc\n\nResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_Dlpla3agsovUqWYMBEtydSRZ', name='sums', type='function_call', id='fc_0b05ac64fb9f2e3300691ba49a52f481a09ac755b15b55aba9', status='completed')\n\n\n\nsource\n\n\ncall_func_openai\n\n call_func_openai (func, ns:Optional[collections.abc.Mapping]=None)\n\n\n\nExported source\ndef call_func_openai(func, ns:Optional[abc.Mapping]=None):\n    return call_func(func.name, json.loads(func.arguments), ns, raise_on_err=False)\n\n\n\nns = mk_ns(sums)\nres = call_func_openai(func, ns=ns)\nres\n\nFinding the sum of 604542 and 6458932\n\n\n7063474\n\n\n\nsource\n\n\nmk_toolres\n\n mk_toolres (r:collections.abc.Mapping,\n             ns:Optional[collections.abc.Mapping]=None)\n\nCreate a tool_result message from response r.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nr\nMapping\n\nResponse containing tool use request\n\n\nns\nOptional\nNone\nNamespace to search for tools\n\n\n\n\n\nExported source\ndef _toolres(r, ns):\n    \"Create a result dict from `tcs`.\"\n    tcs = [o for o in getattr(r, 'output', []) if isinstance(o, ResponseFunctionToolCall)]\n    if ns is None: ns = globals()\n    return { tc.call_id: call_func_openai(tc, ns=mk_ns(ns)) for tc in tcs }\n\n\n\n\nExported source\ndef mk_toolres(\n    r:abc.Mapping, # Response containing tool use request\n    ns:Optional[abc.Mapping]=None # Namespace to search for tools\n    ):\n    \"Create a `tool_result` message from response `r`.\"\n    tr = _toolres(r, ns)\n    r = mk_msg(r)\n    res = [r] if isinstance(r, dict) else listify(r)\n    for k,v in tr.items(): res.append(dict(type=\"function_call_output\", call_id=k, output=str(v)))\n    return res\n\n\n\ntr = mk_toolres(r, ns=ns)\ntr\n\nFinding the sum of 604542 and 6458932\n\n\n[ResponseReasoningItem(id='rs_0b05ac64fb9f2e3300691ba49a185881a0a14fd49b1ba0543b', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_Dlpla3agsovUqWYMBEtydSRZ', name='sums', type='function_call', id='fc_0b05ac64fb9f2e3300691ba49a52f481a09ac755b15b55aba9', status='completed'),\n {'type': 'function_call_output',\n  'call_id': 'call_Dlpla3agsovUqWYMBEtydSRZ',\n  'output': '7063474'}]\n\n\n\nm2 = msgs + tr\n\n\nres = c(mk_msgs(m2), sp=sysp, tools=tools)\nres\n\n604542 + 6,458,932 = 7,063,474\n\n\nid: resp_0b05ac64fb9f2e3300691ba49b810881a0a17737bf23005f54\ncreated_at: 1763419291.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0b05ac64fb9f2e3300691ba49c627481a0b80f4cb5db7b9fbb’, content=[ResponseOutputText(annotations=[], text=‘604542 + 6,458,932 = 7,063,474’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=157, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=177)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\nThis should also work in situations where no tool use is required:\n\nmsgs = mk_toolres(\"I'm Jeremy\")\nc(msgs, sp=sysp, tools=tools, **rkw)\n\nNice to meet you, Jeremy. How can I help you today?\n\n\nid: resp_0aef60f57b7b2a2800691ba49d5ed88195b06058eecd01fcd7\ncreated_at: 1763419293.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0aef60f57b7b2a2800691ba49dd0dc8195accd2091ef50bcef’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0aef60f57b7b2a2800691ba49df7a881959ba9c14b029648f5’, content=[ResponseOutputText(annotations=[], text=‘Nice to meet you, Jeremy. How can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=96, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=116)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nsource\n\n\nClient.structured\n\n Client.structured (msgs:list, tools:Optional[list]=None,\n                    ns:Optional[collections.abc.Mapping]=None, sp:str='',\n                    maxtok=4096, stream:bool=False,\n                    tool_choice:Optional[str]=None, cb:&lt;built-\n                    infunctioncallable&gt;=None,\n                    background:Optional[bool]|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;, conversation:Optional[response_create\n                    _params.Conversation]|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;, include:Optional[List[ResponseIncluda\n                    ble]]|Omit=&lt;openai.Omit object at 0x7f4031b80d90&gt;,\n                    input:Union[str,ResponseInputParam]|Omit=&lt;openai.Omit\n                    object at 0x7f4031b80d90&gt;,\n                    instructions:Optional[str]|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;,\n                    max_output_tokens:Optional[int]|Omit=&lt;openai.Omit\n                    object at 0x7f4031b80d90&gt;,\n                    max_tool_calls:Optional[int]|Omit=&lt;openai.Omit object\n                    at 0x7f4031b80d90&gt;,\n                    metadata:Optional[Metadata]|Omit=&lt;openai.Omit object\n                    at 0x7f4031b80d90&gt;,\n                    model:ResponsesModel|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;,\n                    parallel_tool_calls:Optional[bool]|Omit=&lt;openai.Omit\n                    object at 0x7f4031b80d90&gt;,\n                    previous_response_id:Optional[str]|Omit=&lt;openai.Omit\n                    object at 0x7f4031b80d90&gt;,\n                    prompt:Optional[ResponsePromptParam]|Omit=&lt;openai.Omit\n                    object at 0x7f4031b80d90&gt;,\n                    prompt_cache_key:str|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;,\n                    prompt_cache_retention:\"Optional[Literal['in-\n                    memory','24h']]|Omit\"=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;,\n                    reasoning:Optional[Reasoning]|Omit=&lt;openai.Omit object\n                    at 0x7f4031b80d90&gt;,\n                    safety_identifier:str|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;, service_tier:\"Optional[Literal['auto'\n                    ,'default','flex','scale','priority']]|Omit\"=&lt;openai.O\n                    mit object at 0x7f4031b80d90&gt;,\n                    store:Optional[bool]|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;, stream_options:Optional[response_crea\n                    te_params.StreamOptions]|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;,\n                    temperature:Optional[float]|Omit=&lt;openai.Omit object\n                    at 0x7f4031b80d90&gt;,\n                    text:ResponseTextConfigParam|Omit=&lt;openai.Omit object\n                    at 0x7f4031b80d90&gt;,\n                    top_logprobs:Optional[int]|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;,\n                    top_p:Optional[float]|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;, truncation:\"Optional[Literal['auto','\n                    disabled']]|Omit\"=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;, user:str|Omit=&lt;openai.Omit object at\n                    0x7f4031b80d90&gt;, extra_headers:Headers|None=None,\n                    extra_query:Query|None=None,\n                    extra_body:Body|None=None,\n                    timeout:float|httpx.Timeout|None|NotGiven=NOT_GIVEN)\n\nReturn the value of all tool calls (generally used for structured outputs)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\nlist\n\nPrompt\n\n\ntools\nOptional\nNone\nList of tools to make available to OpenAI model\n\n\nns\nOptional\nNone\nNamespace to search for tools\n\n\nsp\nstr\n\nSystem prompt\n\n\nmaxtok\nint\n4096\nMaximum tokens\n\n\nstream\nbool\nFalse\nStream response?\n\n\ntool_choice\nOptional\nNone\nForced tool choice\n\n\ncb\ncallable\nNone\nCallback after completion\n\n\nbackground\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nconversation\nOptional[response_create_params.Conversation] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ninclude\nOptional[List[ResponseIncludable]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ninput\nUnion[str, ResponseInputParam] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ninstructions\nOptional[str] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmax_output_tokens\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmax_tool_calls\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmetadata\nOptional[Metadata] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmodel\nResponsesModel | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nparallel_tool_calls\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprevious_response_id\nOptional[str] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprompt\nOptional[ResponsePromptParam] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprompt_cache_key\nstr | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprompt_cache_retention\nOptional[Literal[‘in-memory’, ‘24h’]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nreasoning\nOptional[Reasoning] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nsafety_identifier\nstr | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nservice_tier\nOptional[Literal[‘auto’, ‘default’, ‘flex’, ‘scale’, ‘priority’]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nstore\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nstream_options\nOptional[response_create_params.StreamOptions] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntemperature\nOptional[float] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntext\nResponseTextConfigParam | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntop_logprobs\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntop_p\nOptional[float] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntruncation\nOptional[Literal[‘auto’, ‘disabled’]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nuser\nstr | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nextra_headers\nOptional\nNone\nUse the following arguments if you need to pass additional parameters to the API that aren’t available via kwargs.The extra values given here take precedence over values defined on the client or passed to this method.\n\n\nextra_query\nQuery | None\nNone\n\n\n\nextra_body\nBody | None\nNone\n\n\n\ntimeout\nfloat | httpx.Timeout | None | NotGiven\nNOT_GIVEN\n\n\n\n\n\n\nExported source\n@patch\n@delegates(Client.__call__)\ndef structured(self:Client,\n               msgs: list, # Prompt\n               tools:Optional[list]=None, # List of tools to make available to OpenAI model\n               ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n               **kwargs):\n    \"Return the value of all tool calls (generally used for structured outputs)\"\n    if ns is None: ns = mk_ns(tools)\n    r = self(msgs, tools=tools, tool_choice='required', **kwargs)\n    return first(_toolres(r, ns).values())\n\n\n\nclass PrimeMinister(BasicRepr):\n    \"An Australian prime minister\"\n    def __init__(\n        self,\n        firstname:str, # First name\n        surname:str, # Surname\n        dob:str, # Date of birth\n        year_entered:int, # Year first became PM\n    ): store_attr()\n\n\nc1 = Client(model)\nc1.structured('Who was the first prime minister of Australia?', [PrimeMinister], **rkw)\n\nPrimeMinister(firstname='Edmund', surname='Barton', dob='1849-01-18', year_entered=1901)\n\n\n\n\nStreaming tool calling\n\nmsgs = [mk_msg(pr)]\nr = c(msgs, sp=sysp, tools=tools, stream=True, **rkw)\n\nWe can stream back any tool call text (which may be empty):\n\nfor o in r: print(o, end='')\n\nAfter streaming is complete, value.output will contain the tool calls:\n\nr.value.output\n\n[ResponseReasoningItem(id='rs_08c3e97d4ddd09b800691ba4a0aafc8195bd7818f8d7b9ffb4', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_TrwIU2wLr6wVWIaXUwdqIWvg', name='sums', type='function_call', id='fc_08c3e97d4ddd09b800691ba4a0ed808195bb1339b1c870ea41', status='completed')]\n\n\nTherefore we can repeat the same process as before, but using the value attr:\n\ntr = mk_toolres(r.value, ns=ns)\nmsgs += tr\nc(mk_msgs(msgs), sp=sysp, tools=tools, **rkw)\n\nFinding the sum of 604542 and 6458932\n\n\n7063474\n\n\nid: resp_08c3e97d4ddd09b800691ba4a1b1fc8195b365eedb4fb6427f\ncreated_at: 1763419297.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_08c3e97d4ddd09b800691ba4a216448195a412da0131247b6c’, content=[ResponseOutputText(annotations=[], text=‘7063474’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=157, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=7, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=164)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#chat",
    "href": "core.html#chat",
    "title": "Cosette’s source",
    "section": "Chat",
    "text": "Chat\n\nBasic chat\n\nsource\n\n\nChat\n\n Chat (model:Optional[str]=None, cli:Optional[__main__.Client]=None,\n       sp='', tools:Optional[list]=None, hist:list=None,\n       tool_choice:Optional[str]=None,\n       ns:Optional[collections.abc.Mapping]=None, **kw)\n\nOpenAI chat client.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nOptional\nNone\nModel to use (leave empty if passing cli)\n\n\ncli\nOptional\nNone\nClient to use (leave empty if passing model)\n\n\nsp\nstr\n\nOptional system prompt\n\n\ntools\nOptional\nNone\nList of tools to make available\n\n\nhist\nlist\nNone\nInitialize history\n\n\ntool_choice\nOptional\nNone\nForced tool choice\n\n\nns\nOptional\nNone\nNamespace to search for tools\n\n\nkw\nVAR_KEYWORD\n\n\n\n\n\n\n\nExported source\nclass Chat:\n    def __init__(self,\n                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n                 sp='', # Optional system prompt\n                 tools:Optional[list]=None, # List of tools to make available\n                 hist: list = None,  # Initialize history\n                 tool_choice:Optional[str]=None, # Forced tool choice\n                 ns:Optional[abc.Mapping]=None,  # Namespace to search for tools\n                 **kw):\n        \"OpenAI chat client.\"\n        assert model or cli\n        self.c = (cli or Client(model))\n        self.h = hist if hist else []\n        if ns is None: ns=tools\n        self.sp,self.tools,self.tool_choice,self.ns,self.kw = sp,tools,tool_choice,ns,kw\n    \n    @property\n    def use(self): return self.c.use\n\n\n\nchat = Chat(model, sp=sysp, **rkw)\nchat.c.use, chat.h\n\n(In: 0; Out: 0; Total: 0, [])\n\n\n\nsource\n\n\nChat.__call__\n\n Chat.__call__ (pr=None, stream:bool=False, tools=None, tool_choice=None,\n                background:Optional[bool]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;, conversation:Optional[response_create_par\n                ams.Conversation]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;, include:Optional[List[ResponseIncludable]\n                ]|Omit=&lt;openai.Omit object at 0x7f4031b80d90&gt;,\n                input:Union[str,ResponseInputParam]|Omit=&lt;openai.Omit\n                object at 0x7f4031b80d90&gt;,\n                instructions:Optional[str]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;,\n                max_output_tokens:Optional[int]|Omit=&lt;openai.Omit object\n                at 0x7f4031b80d90&gt;,\n                max_tool_calls:Optional[int]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;,\n                metadata:Optional[Metadata]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;, model:ResponsesModel|Omit=&lt;openai.Omit\n                object at 0x7f4031b80d90&gt;,\n                parallel_tool_calls:Optional[bool]|Omit=&lt;openai.Omit\n                object at 0x7f4031b80d90&gt;,\n                previous_response_id:Optional[str]|Omit=&lt;openai.Omit\n                object at 0x7f4031b80d90&gt;,\n                prompt:Optional[ResponsePromptParam]|Omit=&lt;openai.Omit\n                object at 0x7f4031b80d90&gt;,\n                prompt_cache_key:str|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;,\n                prompt_cache_retention:\"Optional[Literal['in-\n                memory','24h']]|Omit\"=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;,\n                reasoning:Optional[Reasoning]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;, safety_identifier:str|Omit=&lt;openai.Omit\n                object at 0x7f4031b80d90&gt;, service_tier:\"Optional[Literal[\n                'auto','default','flex','scale','priority']]|Omit\"=&lt;openai\n                .Omit object at 0x7f4031b80d90&gt;,\n                store:Optional[bool]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;, stream_options:Optional[response_create_p\n                arams.StreamOptions]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;,\n                temperature:Optional[float]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;,\n                text:ResponseTextConfigParam|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;,\n                top_logprobs:Optional[int]|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;, top_p:Optional[float]|Omit=&lt;openai.Omit\n                object at 0x7f4031b80d90&gt;, truncation:\"Optional[Literal['a\n                uto','disabled']]|Omit\"=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;, user:str|Omit=&lt;openai.Omit object at\n                0x7f4031b80d90&gt;, extra_headers:Headers|None=None,\n                extra_query:Query|None=None, extra_body:Body|None=None,\n                timeout:float|httpx.Timeout|None|NotGiven=NOT_GIVEN)\n\nAdd prompt pr to dialog and get a response\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npr\nNoneType\nNone\nPrompt / message\n\n\nstream\nbool\nFalse\nStream response?\n\n\ntools\nNoneType\nNone\nTools to use\n\n\ntool_choice\nNoneType\nNone\nRequired tools to use\n\n\nbackground\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nconversation\nOptional[response_create_params.Conversation] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ninclude\nOptional[List[ResponseIncludable]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ninput\nUnion[str, ResponseInputParam] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ninstructions\nOptional[str] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmax_output_tokens\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmax_tool_calls\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmetadata\nOptional[Metadata] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nmodel\nResponsesModel | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nparallel_tool_calls\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprevious_response_id\nOptional[str] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprompt\nOptional[ResponsePromptParam] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprompt_cache_key\nstr | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nprompt_cache_retention\nOptional[Literal[‘in-memory’, ‘24h’]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nreasoning\nOptional[Reasoning] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nsafety_identifier\nstr | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nservice_tier\nOptional[Literal[‘auto’, ‘default’, ‘flex’, ‘scale’, ‘priority’]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nstore\nOptional[bool] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nstream_options\nOptional[response_create_params.StreamOptions] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntemperature\nOptional[float] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntext\nResponseTextConfigParam | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntop_logprobs\nOptional[int] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntop_p\nOptional[float] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\ntruncation\nOptional[Literal[‘auto’, ‘disabled’]] | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nuser\nstr | Omit\n&lt;openai.Omit object at 0x7f4031b80d90&gt;\n\n\n\nextra_headers\nOptional\nNone\nUse the following arguments if you need to pass additional parameters to the API that aren’t available via kwargs.The extra values given here take precedence over values defined on the client or passed to this method.\n\n\nextra_query\nQuery | None\nNone\n\n\n\nextra_body\nBody | None\nNone\n\n\n\ntimeout\nfloat | httpx.Timeout | None | NotGiven\nNOT_GIVEN\n\n\n\n\n\n\nExported source\n@patch\n@delegates(Responses.create)\ndef __call__(self:Chat,\n             pr=None,  # Prompt / message\n             stream:bool=False, # Stream response?\n             tools=None, # Tools to use\n             tool_choice=None, # Required tools to use\n             **kwargs):\n    \"Add prompt `pr` to dialog and get a response\"\n    if isinstance(pr,str): pr = pr.strip()\n    if pr: self.h.append(mk_msg(pr))\n    if not tools: tools = self.tools\n    if not tool_choice: tool_choice = self.tool_choice\n    kw = self.kw | kwargs\n    def _cb(v):\n        self.last = mk_toolres(v, ns=self.ns)\n        self.h += self.last\n    res = self.c(self.h, sp=self.sp, stream=stream, cb=_cb, tools=tools, **kw)\n    return res\n\n\n\nchat(\"I'm Jeremy\")\nchat(\"What's my name?\")\n\nYour name is Jeremy.\n\n\nid: resp_035c622e9303fba600691ba4a3ec2c8192b023847c0feb78e9\ncreated_at: 1763419299.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_035c622e9303fba600691ba4a48d28819299726cee46729ecb’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_035c622e9303fba600691ba4a4b7188192b44dfccd7d73bcf7’, content=[ResponseOutputText(annotations=[], text=‘Your name is Jeremy.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=68, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=79)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nchat = Chat(model, sp=sysp, **rkw)\nfor o in chat(\"I'm Jeremy\", stream=True): print(o, end='')\n\nNice to meet you, Jeremy. How can I help you today?\n\n\n\nr = chat(\"What's my name?\", stream=True, **rkw)\nfor o in r: print(o, end='')\n\nYour name is Jeremy.\n\n\n\nr.value\n\nYour name is Jeremy.\n\n\nid: resp_08464518fa64a03200691ba4a693808190a7e46bee38560f98\ncreated_at: 1763419302.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_08464518fa64a03200691ba4a71cec819093b98378fa21d647’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_08464518fa64a03200691ba4a743f08190a8c4b9b9f933a435’, content=[ResponseOutputText(annotations=[], text=‘Your name is Jeremy.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=68, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=79)\nuser: None\nstore: True\n\n\n\n\nHistory is stored in the h attr:\n\nchat.h\n\n[{'role': 'user', 'content': \"I'm Jeremy\"},\n ResponseReasoningItem(id='rs_08464518fa64a03200691ba4a5ab60819099386e08686aa1d2', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_08464518fa64a03200691ba4a5ce48819095a23194bdaf6184', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n {'role': 'user', 'content': \"What's my name?\"},\n ResponseReasoningItem(id='rs_08464518fa64a03200691ba4a71cec819093b98378fa21d647', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_08464518fa64a03200691ba4a743f08190a8c4b9b9f933a435', content=[ResponseOutputText(annotations=[], text='Your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\n\n\nChat tool use\n\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\n\nchat = Chat(model, sp=sysp, tools=[sums], **rkw)\nr = chat(pr)\nr.output\n\nFinding the sum of 604542 and 6458932\n\n\n[ResponseReasoningItem(id='rs_0ee933694072489500691ba4a859c081a3a60b193960051eb4', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_ebTRKFMiijgOGVfwFhjH5pPL', name='sums', type='function_call', id='fc_0ee933694072489500691ba4a8a8d881a3b3090c7231d8b92d', status='completed')]\n\n\n\nchat()\n\n7,063,474\n\n\nid: resp_0ee933694072489500691ba4a944e081a39ad25146ac43735a\ncreated_at: 1763419305.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0ee933694072489500691ba4a9bc1081a39dc3eebde73ed1d6’, content=[ResponseOutputText(annotations=[], text=‘7,063,474’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=157, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=9, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=166)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nq = \"In brief, what color flowers are in this image?\"\nchat([img, q])\n\nPurple\n\n\nid: resp_0ee933694072489500691ba4ab556481a388a77a6d4dd0c798\ncreated_at: 1763419307.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0ee933694072489500691ba4abbd5881a3ac733003839637b8’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0ee933694072489500691ba4abeda481a3a4a19dbed1d05c7f’, content=[ResponseOutputText(annotations=[], text=‘Purple’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=257, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=7, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=264)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#third-party-providers",
    "href": "core.html#third-party-providers",
    "title": "Cosette’s source",
    "section": "Third Party Providers",
    "text": "Third Party Providers\n\nAzure OpenAI Service\nExample Azure usage:\nazure_endpoint = AzureOpenAI(\n  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n  api_version=\"2024-08-01-preview\"\n)\n\nclient = Client(models_azure[0], azure_endpoint)\nchat = Chat(cli=client)\nchat(\"Hi.\")",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#other-providers",
    "href": "core.html#other-providers",
    "title": "Cosette’s source",
    "section": "Other providers",
    "text": "Other providers\nHere’s an example of using the library with Groq:\n\ngroq_c = Client(\n    model=\"openai/gpt-oss-20b\",\n    api_key_env=\"GROQ_KEY\",\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\ngroq_c(\"Hello! What's 2+2?\")\n\nHello! 2 + 2 equals 4.\n\n\nid: resp_01kaa21yfhfaja29yr841tz1c9\ncreated_at: 1763421780.0\nerror: None\nincomplete_details: None\ninstructions:\nmetadata: {}\nmodel: openai/gpt-oss-20b\nobject: response\noutput: [ResponseReasoningItem(id=‘resp_01kaa21yfhfajsp0cbvjx0ww5b’, summary=[], type=‘reasoning’, content=[Content(text=‘The user: “Hello! What's 2+2?” It's a straightforward arithmetic question. We need to respond. The policies: no disallowed content. It's fine. Just answer 4. Provide friendly tone.’, type=‘reasoning_text’)], encrypted_content=None, status=‘completed’), ResponseOutputMessage(id=‘msg_01kaa21yfhfak80c1kcggjhmh4’, content=[ResponseOutputText(annotations=[], text=‘Hello! 202f+02f2 equals 4.’, type=‘output_text’, logprobs=None)], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=None)\ntop_logprobs: None\ntruncation: disabled\nusage: ResponseUsage(input_tokens=79, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=66, output_tokens_details=OutputTokensDetails(reasoning_tokens=44), total_tokens=145)\nuser: None\nstore: False\n\n\n\n\n\ngchat = Chat(cli=groq_c)\ngchat(\"Hello! I'm Jeremy\")\n\nHey Jeremy! 👋 How’s it going? Anything exciting on your radar today or something you’d like to chat about?\n\n\nid: resp_01kaa27bsrfxnb5p75bztj84pb\ncreated_at: 1763421957.0\nerror: None\nincomplete_details: None\ninstructions:\nmetadata: {}\nmodel: openai/gpt-oss-20b\nobject: response\noutput: [ResponseReasoningItem(id=‘resp_01kaa27bsrfxnrkywsnwzgd52z’, summary=[], type=‘reasoning’, content=[Content(text=‘The user says “Hello! I'm Jeremy”. They are greeting the assistant. Likely want a friendly response. We should respond politely, maybe ask how they can help. Use friendly tone.’, type=‘reasoning_text’)], encrypted_content=None, status=‘completed’), ResponseOutputMessage(id=‘msg_01kaa27bsrfxpastb7n905j67d’, content=[ResponseOutputText(annotations=[], text=‘Hey Jeremy! 👋 How’s it going? Anything exciting on your radar today or something you’d like to chat about?’, type=‘output_text’, logprobs=None)], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=None)\ntop_logprobs: None\ntruncation: disabled\nusage: ResponseUsage(input_tokens=75, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=73, output_tokens_details=OutputTokensDetails(reasoning_tokens=39), total_tokens=148)\nuser: None\nstore: False\n\n\n\n\n\ngchat(\"What's my name?\")",
    "crumbs": [
      "Cosette's source"
    ]
  }
]